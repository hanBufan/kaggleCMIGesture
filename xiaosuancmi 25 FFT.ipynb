{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":102335,"databundleVersionId":12518947,"sourceType":"competition"},{"sourceId":12139340,"sourceType":"datasetVersion","datasetId":7645099},{"sourceId":12293285,"sourceType":"datasetVersion","datasetId":7748073},{"sourceId":242954653,"sourceType":"kernelVersion"},{"sourceId":246893721,"sourceType":"kernelVersion"}],"dockerImageVersionId":31041,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n<b>\nOnlyInfKernel\n</b></h1> ","metadata":{}},{"cell_type":"code","source":"print(\"hello wrld\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T11:10:03.197439Z","iopub.execute_input":"2025-07-06T11:10:03.198019Z","iopub.status.idle":"2025-07-06T11:10:03.202077Z","shell.execute_reply.started":"2025-07-06T11:10:03.197996Z","shell.execute_reply":"2025-07-06T11:10:03.201372Z"}},"outputs":[{"name":"stdout","text":"hello wrld\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### **ℹ️INFO**\n* First, we would like to thank our participants for sharing their excellent baselines.\n    * [Two‑Branch Human‑Activity‑Recognition Pipeline (IMU + Thermopile/TOF  + SE‑CNN + BiLSTM + Attentio](https://www.kaggle.com/code/vonmainstein/imu-tof)\n\n### **ℹ️IMU+THM/TOF Great Related works(Published)**\n* Thanks for sharing your implementation of the new feature.\n    * LB.75 [CMI25 | IMU+THM/TOF |TF BiLSTM+GRU+Attention|LB.75](https://www.kaggle.com/code/hideyukizushi/cmi25-imu-thm-tof-tf-bilstm-gru-attention-lb-75)\n    * LB.76 [IMU Signal Processing Optimization](https://www.kaggle.com/code/rktqwe/lb-0-76-imu-thm-tof-tf-bilstm-gru-attention)\n    * LB.77 [Gravity Component Removal from Accelerometer Data](https://www.kaggle.com/code/rktqwe/lb-0-77-linear-accel-tf-bilstm-gru-attention)\n    * LB.78 [New Feature: Angular Velocity from Quaternion Derivatives](https://www.kaggle.com/code/nksusth/lb-0-78-quaternions-tf-bilstm-gru-attention)\n\n---\n\n### **ℹ️MyUpdate**\n* In previous notebooks that used \"IMU+TOF\" as features, the validation strategy was TTS, which raised concerns about its robustness.\nFor this reason, we will release an implementation that simply blends five models and a model with a high ValidationScore.\n\n```\npredictions = []\nfor model in models:\n    idx = int(model.predict(pad_input, verbose=0).argmax(1)[0])\n    predictions.append(idx)\n\nidx = max(set(predictions), key=predictions.count)\nreturn str(gesture_classes[idx])\n```\n\n* The validationScore for the local training  model included in this notebook is\n    * [ModelWeight](https://www.kaggle.com/datasets/hideyukizushi/20250627-cmi-b-102-b-105)\n\n```\n0.891134700273056\n0.8912659261884439\n0.8914825129445727\n0.8915471835009202\n0.8922128108549205\n```\n","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n<b>\nModel Detail\n</b></h1> ","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n<b>\nInfPipeline\n</b></h1> ","metadata":{}},{"cell_type":"markdown","source":"### 》》》**Importing the necessary Libraries**","metadata":{}},{"cell_type":"code","source":"import os, json, joblib, numpy as np, pandas as pd\nfrom pathlib import Path\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.utils.class_weight import compute_class_weight\n\nfrom tensorflow.keras.utils import Sequence, to_categorical, pad_sequences\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import (\n    Input, Conv1D, BatchNormalization, Activation, add, MaxPooling1D, Dropout,\n    Bidirectional, LSTM, GlobalAveragePooling1D, Dense, Multiply, Reshape,\n    Lambda, Concatenate, GRU, GaussianNoise\n)\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\nimport polars as pl\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom scipy.spatial.transform import Rotation as R\nimport re\n\n","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-07-06T11:10:03.203108Z","iopub.execute_input":"2025-07-06T11:10:03.203332Z","iopub.status.idle":"2025-07-06T11:10:22.915780Z","shell.execute_reply.started":"2025-07-06T11:10:03.203316Z","shell.execute_reply":"2025-07-06T11:10:22.915134Z"}},"outputs":[{"name":"stderr","text":"2025-07-06 11:10:09.488694: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751800209.760464      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751800209.830614      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### 》》》**Fix Seed**","metadata":{}},{"cell_type":"code","source":"import random\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    tf.experimental.numpy.random.seed(seed)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nseed_everything(seed=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T11:10:22.916789Z","iopub.execute_input":"2025-07-06T11:10:22.917291Z","iopub.status.idle":"2025-07-06T11:10:22.921915Z","shell.execute_reply.started":"2025-07-06T11:10:22.917271Z","shell.execute_reply":"2025-07-06T11:10:22.921172Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### 》》》**Configuration**","metadata":{}},{"cell_type":"code","source":"# (Competition metric will only be imported when TRAINing)\nTRAIN = True                     # ← set to True when you want to train\nRAW_DIR = Path(\"/kaggle/input/cmi-detect-behavior-with-sensor-data\")\nPRETRAINED_DIR = Path(\"/kaggle/input/lb-0-78-quaternions-tf-bilstm-gru-attention\")  # used when TRAIN=False\nEXPORT_DIR = Path(\"./\")                                    # artefacts will be saved here\nBATCH_SIZE = 64\nPAD_PERCENTILE = 95\nLR_INIT = 5e-4\nWD = 3e-3\nMIXUP_ALPHA = 0.4\nEPOCHS = 160\nPATIENCE = 30#40\n\n\nprint(\"▶ imports ready · tensorflow\", tf.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T11:10:22.922841Z","iopub.execute_input":"2025-07-06T11:10:22.923091Z","iopub.status.idle":"2025-07-06T11:10:22.955238Z","shell.execute_reply.started":"2025-07-06T11:10:22.923070Z","shell.execute_reply":"2025-07-06T11:10:22.954490Z"}},"outputs":[{"name":"stdout","text":"▶ imports ready · tensorflow 2.18.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### 》》》**Utility Functions**","metadata":{}},{"cell_type":"code","source":"#Tensor Manipulations\ndef time_sum(x):\n    return K.sum(x, axis=1)\n\ndef squeeze_last_axis(x):\n    return tf.squeeze(x, axis=-1)\n\ndef expand_last_axis(x):\n    return tf.expand_dims(x, axis=-1)\n\ndef se_block(x, reduction=8):\n    ch = x.shape[-1]\n    se = GlobalAveragePooling1D()(x)\n    se = Dense(ch // reduction, activation='relu')(se)\n    se = Dense(ch, activation='sigmoid')(se)\n    se = Reshape((1, ch))(se)\n    return Multiply()([x, se])\n\n# Residual CNN Block with SE\ndef residual_se_cnn_block(x, filters, kernel_size, pool_size=2, drop=0.3, wd=1e-4):\n    shortcut = x\n    for _ in range(2):\n        x = Conv1D(filters, kernel_size, padding='same', use_bias=False,\n                   kernel_regularizer=l2(wd))(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n    x = se_block(x)\n    if shortcut.shape[-1] != filters:\n        shortcut = Conv1D(filters, 1, padding='same', use_bias=False,\n                          kernel_regularizer=l2(wd))(shortcut)\n        shortcut = BatchNormalization()(shortcut)\n    x = add([x, shortcut])\n    x = Activation('relu')(x)\n    x = MaxPooling1D(pool_size)(x)\n    x = Dropout(drop)(x)\n    return x\n\ndef attention_layer(inputs):\n    score = Dense(1, activation='tanh')(inputs)\n    score = Lambda(squeeze_last_axis)(score)\n    weights = Activation('softmax')(score)\n    weights = Lambda(expand_last_axis)(weights)\n    context = Multiply()([inputs, weights])\n    context = Lambda(time_sum)(context)\n    return context","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T11:10:22.957020Z","iopub.execute_input":"2025-07-06T11:10:22.957362Z","iopub.status.idle":"2025-07-06T11:10:22.972865Z","shell.execute_reply.started":"2025-07-06T11:10:22.957337Z","shell.execute_reply":"2025-07-06T11:10:22.972184Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### 》》》**Data Helpers**","metadata":{}},{"cell_type":"code","source":"# Normalizes and cleans the time series sequence. \n\ndef preprocess_sequence(df_seq: pd.DataFrame, feature_cols: list[str], scaler: StandardScaler):\n    mat = df_seq[feature_cols].ffill().bfill().fillna(0).values\n    return scaler.transform(mat).astype('float32')\n\n# MixUp the data argumentation in order to regularize the neural network. \n\nclass MixupGenerator(Sequence):\n    def __init__(self, X, y, batch_size, alpha=0.2):\n        self.X, self.y = X, y\n        self.batch = batch_size\n        self.alpha = alpha\n        self.indices = np.arange(len(X))\n    def __len__(self):\n        return int(np.ceil(len(self.X) / self.batch))\n    def __getitem__(self, i):\n        idx = self.indices[i*self.batch:(i+1)*self.batch]\n        Xb, yb = self.X[idx], self.y[idx]\n        lam = np.random.beta(self.alpha, self.alpha)\n        perm = np.random.permutation(len(Xb))\n        X_mix = lam * Xb + (1-lam) * Xb[perm]\n        y_mix = lam * yb + (1-lam) * yb[perm]\n        return X_mix, y_mix\n    def on_epoch_end(self):\n        np.random.shuffle(self.indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T11:10:22.973606Z","iopub.execute_input":"2025-07-06T11:10:22.973879Z","iopub.status.idle":"2025-07-06T11:10:22.991171Z","shell.execute_reply.started":"2025-07-06T11:10:22.973863Z","shell.execute_reply":"2025-07-06T11:10:22.990492Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def remove_gravity_from_acc(acc_data, rot_data):\n\n    if isinstance(acc_data, pd.DataFrame):\n        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n    else:\n        acc_values = acc_data\n\n    if isinstance(rot_data, pd.DataFrame):\n        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n    else:\n        quat_values = rot_data\n\n    num_samples = acc_values.shape[0]\n    linear_accel = np.zeros_like(acc_values)\n    \n    gravity_world = np.array([0, 0, 9.81])\n\n    for i in range(num_samples):\n        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n            linear_accel[i, :] = acc_values[i, :] \n            continue\n\n        try:\n            rotation = R.from_quat(quat_values[i])\n            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n        except ValueError:\n             linear_accel[i, :] = acc_values[i, :]\n             \n    return linear_accel\n\ndef calculate_angular_velocity_from_quat(rot_data, time_delta=1/200): # Assuming 200Hz sampling rate\n    if isinstance(rot_data, pd.DataFrame):\n        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n    else:\n        quat_values = rot_data\n\n    num_samples = quat_values.shape[0]\n    angular_vel = np.zeros((num_samples, 3))\n\n    for i in range(num_samples - 1):\n        q_t = quat_values[i]\n        q_t_plus_dt = quat_values[i+1]\n\n        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n            continue\n\n        try:\n            rot_t = R.from_quat(q_t)\n            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n\n            # Calculate the relative rotation\n            delta_rot = rot_t.inv() * rot_t_plus_dt\n            \n            # Convert delta rotation to angular velocity vector\n            # The rotation vector (Euler axis * angle) scaled by 1/dt\n            # is a good approximation for small delta_rot\n            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n        except ValueError:\n            # If quaternion is invalid, angular velocity remains zero\n            pass\n            \n    return angular_vel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T11:10:22.991949Z","iopub.execute_input":"2025-07-06T11:10:22.992208Z","iopub.status.idle":"2025-07-06T11:10:23.009098Z","shell.execute_reply.started":"2025-07-06T11:10:22.992185Z","shell.execute_reply":"2025-07-06T11:10:23.008543Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## *v20: added FFT and freq analysis to feature engineering*\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom scipy.stats import entropy\nimport matplotlib.pyplot as plt\n\n# 假设 fs 是采样率\nfs = 100  # 设置采样率为 100Hz，当然根据你的数据来调整\n\ndef FFTforLinearAccel(linear_accel):\n    \"\"\"\n    对加速度的 x, y, z 分量分别进行傅里叶变换，并返回各个分量的 FFT 结果。\n    \n    参数:\n    - linear_accel: 三维加速度数据，形状为 (n_samples, 3)，每一列对应 x, y, z 方向的加速度。\n    \n    返回:\n    - fft_results: 包含 x, y, z 分量 FFT 结果的字典。\n    \"\"\"\n    accelX = linear_accel[:, 0]  # x方向的加速度\n    accelY = linear_accel[:, 1]  # y方向的加速度\n    accelZ = linear_accel[:, 2]  # z方向的加速度\n    \n    fft_resultX = np.fft.fft(accelX)  # 对加速度数据做傅里叶变换\n    freqsX = np.fft.fftfreq(len(accelX), d=1/fs)  # 计算对应的频率轴\n\n    fft_resultY = np.fft.fft(accelY)  # y方向的傅里叶变换\n    freqsY = np.fft.fftfreq(len(accelY), d=1/fs)\n\n    fft_resultZ = np.fft.fft(accelZ)  # z方向的傅里叶变换\n    freqsZ = np.fft.fftfreq(len(accelZ), d=1/fs)\n\n    fft_results = {\n        'x': (fft_resultX, freqsX),\n        'y': (fft_resultY, freqsY),\n        'z': (fft_resultZ, freqsZ)\n    }\n    return fft_results\n\n\ndef freqEntropyFromLinearAccelFFT(fft_results):\n    \"\"\"\n    计算从傅里叶变换结果得到的频域熵。\n    \n    参数:\n    - fft_results: 包含傅里叶变换结果和频率轴的字典。\n    \n    返回:\n    - spectral_entropy: 频域熵值。\n    \"\"\"\n    # 计算幅度\n    amplitude_x = np.abs(fft_results['x'][0])\n    amplitude_y = np.abs(fft_results['y'][0])\n    amplitude_z = np.abs(fft_results['z'][0])\n\n    # 归一化幅度\n    amplitude_x_norm = amplitude_x / np.sum(amplitude_x)\n    amplitude_y_norm = amplitude_y / np.sum(amplitude_y)\n    amplitude_z_norm = amplitude_z / np.sum(amplitude_z)\n    \n    # 计算频域熵\n    spectral_entropy_x = entropy(amplitude_x_norm)\n    spectral_entropy_y = entropy(amplitude_y_norm)\n    spectral_entropy_z = entropy(amplitude_z_norm)\n    '''\n    print(f\"Spectral Entropy for X: {spectral_entropy_x}\")\n    print(f\"Spectral Entropy for Y: {spectral_entropy_y}\")\n    print(f\"Spectral Entropy for Z: {spectral_entropy_z}\")\n    '''\n    \n    \n    return spectral_entropy_x, spectral_entropy_y, spectral_entropy_z\n'''\n\n# 测试：生成一组 dummy 数据并进行验证\nnp.random.seed(42)  # 保证结果可复现\ndummy_data = np.random.randn(1000, 3)  # 生成 1000 个随机数据点，模拟加速度（x, y, z）\n# 对加速度数据进行傅里叶变换并计算频域熵\nfft_results = FFTforLinearAccel(dummy_data)\nspectral_entropy = freqEntropyFromLinearAccelFFT(fft_results)\n\n\n'''\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T11:10:23.010023Z","iopub.execute_input":"2025-07-06T11:10:23.010238Z","iopub.status.idle":"2025-07-06T11:10:23.029965Z","shell.execute_reply.started":"2025-07-06T11:10:23.010222Z","shell.execute_reply":"2025-07-06T11:10:23.029216Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'\\n\\n# 测试：生成一组 dummy 数据并进行验证\\nnp.random.seed(42)  # 保证结果可复现\\ndummy_data = np.random.randn(1000, 3)  # 生成 1000 个随机数据点，模拟加速度（x, y, z）\\n# 对加速度数据进行傅里叶变换并计算频域熵\\nfft_results = FFTforLinearAccel(dummy_data)\\nspectral_entropy = freqEntropyFromLinearAccelFFT(fft_results)\\n\\n\\n'"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"def calculate_angular_distance(rot_data):\n    if isinstance(rot_data, pd.DataFrame):\n        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n    else:\n        quat_values = rot_data\n\n    num_samples = quat_values.shape[0]\n    angular_dist = np.zeros(num_samples)\n\n    for i in range(num_samples - 1):\n        q1 = quat_values[i]\n        q2 = quat_values[i+1]\n\n        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n            angular_dist[i] = 0 # Or np.nan, depending on desired behavior\n            continue\n        try:\n            # Conversion of quaternions to Rotation objects\n            r1 = R.from_quat(q1)\n            r2 = R.from_quat(q2)\n\n            # Calculating angular distance: 2 * arccos(|real(p * q*)|)\n            # where p* is the conjugate quaternion of q\n            # In scipy.spatial.transform.Rotation, r1.inv() * r2 gives the relative rotation.\n            # The angle of this relative rotation is the angular distance.\n            relative_rotation = r1.inv() * r2\n            \n            # The angle of the rotation vector corresponds to the angular distance\n            # The norm of the rotation vector is the angle in radians\n            angle = np.linalg.norm(relative_rotation.as_rotvec())\n            angular_dist[i] = angle\n        except ValueError:\n            angular_dist[i] = 0 # In case of invalid quaternions\n            pass\n            \n    return angular_dist\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T11:10:23.030684Z","iopub.execute_input":"2025-07-06T11:10:23.030940Z","iopub.status.idle":"2025-07-06T11:10:23.050394Z","shell.execute_reply.started":"2025-07-06T11:10:23.030924Z","shell.execute_reply":"2025-07-06T11:10:23.049694Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import re\nfrom scipy.spatial.transform import Rotation as R\nimport numpy as np\n\ndef equivalentToSittingStraight(linear_accel, orientation):\n    \"\"\"\n    将加速度数据转换为坐直（sitting straight）姿势下的加速度。\n    \n    参数：\n    - linear_accel: 三维加速度数据（如经过重力去除后的平动加速度）。\n    - orientation: 当前的姿势信息（如 \"seated lean\", \"seated straight\", \"lie on side\" 等）。\n    \n    返回：\n    - 转换为坐正姿势后的加速度数据。\n    \"\"\"\n    # 检查 \"seated lean\" 姿势\n    if re.search(r'\\bseated\\s*lean\\b', orientation, re.IGNORECASE):\n        rotation = R.from_euler('xyz', [0, -30, 0], degrees=True)  # 负的 Pitch\n    # 检查 \"seated straight\" 姿势\n    elif re.search(r'\\bseated\\s*straight\\b', orientation, re.IGNORECASE):\n        rotation = R.from_euler('xyz', [0, 0, 0], degrees=True)  # 没有旋转\n    # 检查 \"lie on side\" 姿势\n    elif re.search(r'\\blie\\s*on\\s*side\\b', orientation, re.IGNORECASE):\n        rotation = R.from_euler('xyz', [90, 90, 0], degrees=True)  # 90° Roll 和 90° Pitch\n    # 检查 \"lie on back\" 姿势\n    elif re.search(r'\\blie\\s*on\\s*back\\b', orientation, re.IGNORECASE):\n        rotation = R.from_euler('xyz', [0, 90, 0], degrees=True)  # 90° Pitch\n    else:\n        # 如果姿势没有匹配，返回原始加速度\n        rotation = R.from_euler('xyz', [0, 0, 0], degrees=True)  # 默认无旋转\n    \n    # 获取旋转矩阵的逆\n    inverseRotation = rotation.inv().as_matrix()\n\n    # 确保 linear_accel 是三维向量（例如形状为 (3,) 或 (n, 3)）\n    # 如果 linear_accel 是一个 1D 数组，转换为 (3, ) 的 3D 向量\n    if linear_accel.ndim == 1:\n        linear_accel = linear_accel.reshape(1, 3)  # 将其转为 1 x 3 数组\n\n    # 使用逆旋转矩阵对加速度进行转换\n    adjusted_accel = inverseRotation @ linear_accel.T  # 矩阵乘法，确保形状匹配\n\n    return adjusted_accel.T  # 返回转换后的加速度数据\n\n'''\n# 例子\nlinear_accel = np.array([0.1, 0.2, 0.3])  # 三维加速度向量\norientation = 'seated lean'\n\nadjusted_accel = equivalentToSittingStraight(linear_accel, orientation)\nprint(f\"Adjusted Linear Acceleration: {adjusted_accel}\")\n\n'''\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T11:10:23.051094Z","iopub.execute_input":"2025-07-06T11:10:23.051274Z","iopub.status.idle":"2025-07-06T11:10:23.077736Z","shell.execute_reply.started":"2025-07-06T11:10:23.051260Z","shell.execute_reply":"2025-07-06T11:10:23.076919Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'\\n# 例子\\nlinear_accel = np.array([0.1, 0.2, 0.3])  # 三维加速度向量\\norientation = \\'seated lean\\'\\n\\nadjusted_accel = equivalentToSittingStraight(linear_accel, orientation)\\nprint(f\"Adjusted Linear Acceleration: {adjusted_accel}\")\\n\\n'"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"### 》》》**Model Definition - Two Branch Architecture**","metadata":{}},{"cell_type":"code","source":"def build_two_branch_model(pad_len, imu_dim, tof_dim, n_classes, wd=1e-4):\n    inp = Input(shape=(pad_len, imu_dim+tof_dim))\n    imu = Lambda(lambda t: t[:, :, :imu_dim])(inp)\n    tof = Lambda(lambda t: t[:, :, imu_dim:])(inp)\n\n    # IMU deep branch\n    x1 = residual_se_cnn_block(imu, 64, 3, drop=0.1, wd=wd)\n    x1 = residual_se_cnn_block(x1, 128, 5, drop=0.1, wd=wd)\n\n    # TOF/Thermal lighter branch\n    x2 = Conv1D(64, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(tof)\n    x2 = BatchNormalization()(x2); x2 = Activation('relu')(x2)\n    x2 = MaxPooling1D(2)(x2); x2 = Dropout(0.2)(x2)\n    x2 = Conv1D(128, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x2)\n    x2 = BatchNormalization()(x2); x2 = Activation('relu')(x2)\n    x2 = MaxPooling1D(2)(x2); x2 = Dropout(0.2)(x2)\n\n    merged = Concatenate()([x1, x2])\n\n    xa = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n    xb = Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n    xc = GaussianNoise(0.09)(merged)\n    xc = Dense(16, activation='elu')(xc)\n    \n    x = Concatenate()([xa, xb, xc])\n    x = Dropout(0.4)(x)\n    x = attention_layer(x)\n\n    for units, drop in [(256, 0.5), (128, 0.3)]:\n        x = Dense(units, use_bias=False, kernel_regularizer=l2(wd))(x)\n        x = BatchNormalization()(x); x = Activation('relu')(x)\n        x = Dropout(drop)(x)\n\n    out = Dense(n_classes, activation='softmax', kernel_regularizer=l2(wd))(x)\n    return Model(inp, out)\n\ntmp_model = build_two_branch_model(127,7,325,18)","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-07-06T11:10:23.079539Z","iopub.execute_input":"2025-07-06T11:10:23.079765Z","iopub.status.idle":"2025-07-06T11:10:26.510081Z","shell.execute_reply.started":"2025-07-06T11:10:23.079749Z","shell.execute_reply":"2025-07-06T11:10:26.509489Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1751800224.538767      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"### 》》》**Training / Inference Pipeline**","metadata":{}},{"cell_type":"code","source":"custom_objs = {\n    'time_sum': time_sum,\n    'squeeze_last_axis': squeeze_last_axis,\n    'expand_last_axis': expand_last_axis,\n    'se_block': se_block,\n    'residual_se_cnn_block': residual_se_cnn_block,\n    'attention_layer': attention_layer,\n}\n\nprint(\"hello world custom objects\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T11:10:26.510797Z","iopub.execute_input":"2025-07-06T11:10:26.511048Z","iopub.status.idle":"2025-07-06T11:10:26.515893Z","shell.execute_reply.started":"2025-07-06T11:10:26.511031Z","shell.execute_reply":"2025-07-06T11:10:26.515100Z"}},"outputs":[{"name":"stdout","text":"hello world custom objects\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"\nif TRAIN:\n    print(\"▶ TRAIN MODE – loading dataset …\")\n    df = pd.read_csv(RAW_DIR / \"train.csv\")\n\n    train_dem_df = pd.read_csv(RAW_DIR / \"train_demographics.csv\")\n    df_for_groups = pd.merge(df.copy(), train_dem_df, on='subject', how='left')\n\n    le = LabelEncoder()\n    df['gesture_int'] = le.fit_transform(df['gesture'])\n    np.save(EXPORT_DIR / \"gesture_classes.npy\", le.classes_)\n    gesture_classes = le.classes_\n\n    print(\"  Calculating base engineered IMU features (magnitude, angle)...\")\n    df['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n    df['rot_angle'] = 2 * np.arccos(df['rot_w'].clip(-1, 1))\n    \n    print(\"  Calculating engineered IMU derivatives (jerk, angular velocity) for original acc_mag...\")\n    df['acc_mag_jerk'] = df.groupby('sequence_id')['acc_mag'].diff().fillna(0)\n    df['rot_angle_vel'] = df.groupby('sequence_id')['rot_angle'].diff().fillna(0)\n\n    print(\"  Removing gravity and calculating linear acceleration features...\")\n\n    linear_accel_list = []\n    \n    freq_entropy_x_list = []\n    freq_entropy_y_list = []\n    freq_entropy_z_list = []\n\n    spectral_flatness_x_list = []\n    spectral_flatness_y_list = []\n    spectral_flatness_z_list = []\n\n    for _, group in df.groupby('sequence_id'):\n        acc_data_group = group[['acc_x', 'acc_y', 'acc_z']]\n        rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n        pose = group['orientation'].iloc[0]\n        linear_accel_group = remove_gravity_from_acc(acc_data_group, rot_data_group)\n        #print(\"preTarget\")\n        linear_accel_group = equivalentToSittingStraight (linear_accel_group, pose)\n        #print(\"postTarget\")\n        linear_accel_list.append(pd.DataFrame(linear_accel_group, columns=['linear_acc_x', 'linear_acc_y', 'linear_acc_z'], index=group.index))\n\n        fft_results = FFTforLinearAccel(linear_accel_group)\n\n        freq_entropy_x, freq_entropy_y, freq_entropy_z = freqEntropyFromLinearAccelFFT(fft_results)\n    \n\n        freq_entropy_x_list.append(freq_entropy_x)\n        freq_entropy_y_list.append(freq_entropy_y)\n        freq_entropy_z_list.append(freq_entropy_z)\n    \n\n        print(\"FFT freq analysis of linear acceleration\")\n        \n        amplitude_x = np.abs(fft_results['x'][0])\n        amplitude_y = np.abs(fft_results['y'][0])\n        amplitude_z = np.abs(fft_results['z'][0])\n\n        \n        print(\"Freq entropy from lindear acceleration\")\n            \n        print(\"Spectral flatness\")\n        spectral_flatness_x = np.exp(np.mean(np.log(amplitude_x + 1e-8))) / np.mean(amplitude_x)\n        spectral_flatness_y = np.exp(np.mean(np.log(amplitude_y + 1e-8))) / np.mean(amplitude_y)\n        spectral_flatness_z = np.exp(np.mean(np.log(amplitude_z + 1e-8))) / np.mean(amplitude_z)\n\n        \n        \n        spectral_flatness_x_list.append(spectral_flatness_x)\n        spectral_flatness_y_list.append(spectral_flatness_y)\n        spectral_flatness_z_list.append(spectral_flatness_z)\n\n\n    print(freq_entropy_x_list)\n    df_linear_accel = pd.concat(linear_accel_list)\n    df = pd.concat([df, df_linear_accel], axis=1)\n\n    print\n    df['freq_entropy_x'] = pd.concat([pd.Series(x) for x in freq_entropy_x_list], axis=0).reset_index(drop=True)\n    df['freq_entropy_y'] = pd.concat([pd.Series(x) for x in freq_entropy_y_list], axis=0).reset_index(drop=True)\n    df['freq_entropy_z'] = pd.concat([pd.Series(x) for x in freq_entropy_z_list], axis=0).reset_index(drop=True)\n\n\n    # Convert each float in the lists to a pandas Series before concatenating\n    '''\n\n    df['spectral_flatness_x'] = pd.concat([pd.Series([x]) for x in spectral_flatness_x_list], axis=0).reset_index(drop=True)\n    df['spectral_flatness_y'] = pd.concat([pd.Series([x]) for x in spectral_flatness_y_list], axis=0).reset_index(drop=True)\n    df['spectral_flatness_z'] = pd.concat([pd.Series([x]) for x in spectral_flatness_z_list], axis=0).reset_index(drop=True)\n\n    \n    '''\n   \n\n    df['linear_acc_mag'] = np.sqrt(df['linear_acc_x']**2 + df['linear_acc_y']**2 + df['linear_acc_z']**2)\n    df['linear_acc_mag_jerk'] = df.groupby('sequence_id')['linear_acc_mag'].diff().fillna(0)\n\n    print(\"  Calculating angular velocity from quaternion derivatives...\")\n    angular_vel_list = []\n    for _, group in df.groupby('sequence_id'):\n        rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n        angular_vel_group = calculate_angular_velocity_from_quat(rot_data_group)\n        angular_vel_list.append(pd.DataFrame(angular_vel_group, columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'], index=group.index))\n    \n    df_angular_vel = pd.concat(angular_vel_list)\n    df = pd.concat([df, df_angular_vel], axis=1)\n\n    print(\"  Calculating angular distance between successive quaternions...\")\n    angular_distance_list = []\n    for _, group in df.groupby('sequence_id'):\n        rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n        angular_dist_group = calculate_angular_distance(rot_data_group)\n        angular_distance_list.append(pd.DataFrame(angular_dist_group, columns=['angular_distance'], index=group.index))\n    \n    df_angular_distance = pd.concat(angular_distance_list)\n    df = pd.concat([df, df_angular_distance], axis=1)\n\n    meta_cols = { } # This was an empty dict in your provided code, keeping it as is.\n\n    imu_cols_base = ['linear_acc_x', 'linear_acc_y', 'linear_acc_z']\n    imu_cols_base.extend([c for c in df.columns if c.startswith('rot_') and c not in ['rot_angle', 'rot_angle_vel']])\n    \n    imu_engineered_features = [\n        'acc_mag', 'rot_angle',\n        'acc_mag_jerk', 'rot_angle_vel',\n        'linear_acc_mag', 'linear_acc_mag_jerk',\n        'angular_vel_x', 'angular_vel_y', 'angular_vel_z', # Existing new features\n        'angular_distance', # Added new feature\n        'freq_entropy_x',# freq entropy\n        'freq_entropy_y',\n        'freq_entropy_z'\n    \n        '''\n         'spectral_flatness_x',#freq flatness\n        'spectral_flatness_y',\n        'spectral_flatness_z'\n        \n        '''\n        \n       \n        \n        \n    ]\n    imu_cols = imu_cols_base + imu_engineered_features\n    imu_cols = list(dict.fromkeys(imu_cols)) # Для удаления дубликатов\n\n    thm_cols_original = [c for c in df.columns if c.startswith('thm_')]\n    \n    tof_aggregated_cols_template = []\n    for i in range(1, 6):\n        tof_aggregated_cols_template.extend([f'tof_{i}_mean', f'tof_{i}_std', f'tof_{i}_min', f'tof_{i}_max'])\n\n    final_feature_cols = imu_cols + thm_cols_original + tof_aggregated_cols_template\n    imu_dim_final = len(imu_cols)\n    tof_thm_aggregated_dim_final = len(thm_cols_original) + len(tof_aggregated_cols_template)\n    \n    print(f\"  IMU (incl. engineered & derivatives) {imu_dim_final} | THM + Aggregated TOF {tof_thm_aggregated_dim_final} | total {len(final_feature_cols)} features\")\n    np.save(EXPORT_DIR / \"feature_cols.npy\", np.array(final_feature_cols))\n\n    print(\"  Building sequences with aggregated TOF and preparing data for scaler...\")\n    seq_gp = df.groupby('sequence_id') \n    \n    all_steps_for_scaler_list = []\n    X_list_unscaled, y_list_int_for_stratify, lens = [], [], [] \n\n    for seq_id, seq_df_orig in seq_gp:\n        seq_df = seq_df_orig.copy()\n\n        for i in range(1, 6):\n            pixel_cols_tof = [f\"tof_{i}_v{p}\" for p in range(64)]\n            tof_sensor_data = seq_df[pixel_cols_tof].replace(-1, np.nan)\n            seq_df[f'tof_{i}_mean'] = tof_sensor_data.mean(axis=1)\n            seq_df[f'tof_{i}_std']  = tof_sensor_data.std(axis=1)\n            seq_df[f'tof_{i}_min']  = tof_sensor_data.min(axis=1)\n            seq_df[f'tof_{i}_max']  = tof_sensor_data.max(axis=1)\n        \n        mat_unscaled = seq_df[final_feature_cols].ffill().bfill().fillna(0).values.astype('float32')\n        \n        all_steps_for_scaler_list.append(mat_unscaled)\n        X_list_unscaled.append(mat_unscaled)\n        y_list_int_for_stratify.append(seq_df['gesture_int'].iloc[0])\n        lens.append(len(mat_unscaled))\n\n    print(\"  Fitting StandardScaler...\")\n    all_steps_concatenated = np.concatenate(all_steps_for_scaler_list, axis=0)\n    scaler = StandardScaler().fit(all_steps_concatenated)\n    joblib.dump(scaler, EXPORT_DIR / \"scaler.pkl\")\n    del all_steps_for_scaler_list, all_steps_concatenated\n\n    print(\"  Scaling and padding sequences...\")\n    X_scaled_list = [scaler.transform(x_seq) for x_seq in X_list_unscaled]\n    del X_list_unscaled\n\n    pad_len = int(np.percentile(lens, PAD_PERCENTILE))\n    np.save(EXPORT_DIR / \"sequence_maxlen.npy\", pad_len)\n    \n    X = pad_sequences(X_scaled_list, maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n    del X_scaled_list\n    \n    y_int_for_stratify = np.array(y_list_int_for_stratify)\n    y = to_categorical(y_int_for_stratify, num_classes=len(le.classes_))\n\n    print(\"  Splitting data and preparing for training...\")\n    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=82, stratify=y_int_for_stratify)\n\n    cw_vals = compute_class_weight('balanced', classes=np.arange(len(le.classes_)), y=y_int_for_stratify)\n    class_weight = dict(enumerate(cw_vals))\n\n    model = build_two_branch_model(pad_len, imu_dim_final, tof_thm_aggregated_dim_final, len(le.classes_), wd=WD)\n    \n    steps = len(X_tr) // BATCH_SIZE\n    lr_sched = tf.keras.optimizers.schedules.CosineDecayRestarts(5e-4, first_decay_steps=15 * steps) \n    \n    model.compile(optimizer=Adam(lr_sched),\n                  loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n                  metrics=['accuracy'])\n\n    train_gen = MixupGenerator(X_tr, y_tr, batch_size=BATCH_SIZE, alpha=MIXUP_ALPHA)\n    cb = EarlyStopping(patience=PATIENCE, restore_best_weights=True, verbose=0, monitor='val_accuracy', mode='max')\n    \n    print(\"  Starting model training...\")\n    model.fit(train_gen, epochs=EPOCHS, validation_data=(X_val, y_val),\n              class_weight=class_weight, callbacks=[cb], verbose=1)\n\n    model.save(EXPORT_DIR / \"gesture_two_branch_mixup.h5\")\n    print(\"✔ Training done – artefacts saved in\", EXPORT_DIR)\n\n    from cmi_2025_metric_copy_for_import import CompetitionMetric\n    preds_val = model.predict(X_val).argmax(1)\n    true_val_int  = y_val.argmax(1)\n    \n    h_f1 = CompetitionMetric().calculate_hierarchical_f1(\n        pd.DataFrame({'gesture': le.classes_[true_val_int]}),\n        pd.DataFrame({'gesture': le.classes_[preds_val]}))\n    print(\"Hold‑out H‑F1 =\", round(h_f1, 4))\nelse:\n    print(\"▶ INFERENCE MODE – loading artefacts from\", PRETRAINED_DIR)\n    final_feature_cols = np.load(PRETRAINED_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n    pad_len        = int(np.load(PRETRAINED_DIR / \"sequence_maxlen.npy\"))\n    scaler         = joblib.load(PRETRAINED_DIR / \"scaler.pkl\")\n    gesture_classes = np.load(PRETRAINED_DIR / \"gesture_classes.npy\", allow_pickle=True)\n\n    # Re-calculate imu_dim_final based on the actual features that will be used\n    # Убедитесь, что 'angular_distance' учитывается здесь при инференсе\n    imu_features_in_final_cols = [c for c in final_feature_cols if any(c.startswith(prefix) for prefix in ['linear_acc_', 'acc_', 'rot_', 'angular_vel_', 'angular_distance'])]\n    imu_dim_final = len(imu_features_in_final_cols)\n\n    tof_thm_aggregated_dim_final = len(final_feature_cols) - imu_dim_final\n\n   \n    \n    model = load_model(PRETRAINED_DIR / \"gesture_two_branch_mixup.h5\",\n                       compile=False, custom_objects=custom_objs)\n    print(\"  Model, scaler, feature_cols, pad_len loaded – ready for evaluation\")","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-07-06T11:10:26.516957Z","iopub.execute_input":"2025-07-06T11:10:26.517194Z","iopub.status.idle":"2025-07-06T11:10:27.118854Z","shell.execute_reply.started":"2025-07-06T11:10:26.517177Z","shell.execute_reply":"2025-07-06T11:10:27.118199Z"}},"outputs":[{"name":"stdout","text":"▶ INFERENCE MODE – loading artefacts from /kaggle/input/lb-0-78-quaternions-tf-bilstm-gru-attention\n  Model, scaler, feature_cols, pad_len loaded – ready for evaluation\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n<b>\nBlending Model\n</b></h1> ","metadata":{}},{"cell_type":"markdown","source":"### 》》》**LoadBlendingModels**","metadata":{}},{"cell_type":"code","source":"models = []\n\nmodel = load_model(\"/kaggle/input/lb-0-78-quaternions-tf-bilstm-gru-attention/gesture_two_branch_mixup.h5\",compile=False, custom_objects=custom_objs)\nmodels.append(model)\n\n\nmodel = load_model(\"/kaggle/input/20250627-cmi-b-102-b-105/0.8914825129445727_.h5\",compile=False, custom_objects=custom_objs)\nmodels.append(model)\nmodel = load_model(\"/kaggle/input/20250627-cmi-b-102-b-105/0.8912659261884439_.h5\",compile=False, custom_objects=custom_objs)\nmodels.append(model)\nmodel = load_model(\"/kaggle/input/20250627-cmi-b-102-b-105/0.891134700273056_.h5\",compile=False, custom_objects=custom_objs)\nmodels.append(model)\nmodel = load_model(\"/kaggle/input/20250627-cmi-b-102-b-105/0.8915471835009202_.h5\",compile=False, custom_objects=custom_objs)\nmodels.append(model)\nmodel = load_model(\"/kaggle/input/20250627-cmi-b-102-b-105/0.8922128108549205_.h5\",compile=False, custom_objects=custom_objs)\nmodels.append(model)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T11:10:27.119749Z","iopub.execute_input":"2025-07-06T11:10:27.120015Z","iopub.status.idle":"2025-07-06T11:10:30.170832Z","shell.execute_reply.started":"2025-07-06T11:10:27.119992Z","shell.execute_reply":"2025-07-06T11:10:30.170023Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### 》》》**Predict**","metadata":{}},{"cell_type":"code","source":"def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n    df_seq = sequence.to_pandas()\n\n    df_seq['acc_mag'] = np.sqrt(df_seq['acc_x']**2 + df_seq['acc_y']**2 + df_seq['acc_z']**2)\n    df_seq['rot_angle'] = 2 * np.arccos(df_seq['rot_w'].clip(-1, 1))\n    df_seq['acc_mag_jerk'] = df_seq['acc_mag'].diff().fillna(0)\n    df_seq['rot_angle_vel'] = df_seq['rot_angle'].diff().fillna(0)\n\n    \n\n    acc_cols_for_gravity_removal = ['acc_x', 'acc_y', 'acc_z']\n    rot_cols_for_gravity_removal = ['rot_x', 'rot_y', 'rot_z', 'rot_w']\n\n    if not all(col in df_seq.columns for col in acc_cols_for_gravity_removal + rot_cols_for_gravity_removal):\n        print(f\"Warning: Missing raw acc/rot columns for gravity removal in predict for sequence. Using raw acc as linear.\")\n        df_seq['linear_acc_x'] = df_seq.get('acc_x', 0)\n        df_seq['linear_acc_y'] = df_seq.get('acc_y', 0)\n        df_seq['linear_acc_z'] = df_seq.get('acc_z', 0)\n    else:\n        acc_data_seq = df_seq[acc_cols_for_gravity_removal]\n        rot_data_seq = df_seq[rot_cols_for_gravity_removal]\n        linear_accel_seq_arr = remove_gravity_from_acc(acc_data_seq, rot_data_seq)\n        \n        df_seq['linear_acc_x'] = linear_accel_seq_arr[:, 0]\n        df_seq['linear_acc_y'] = linear_accel_seq_arr[:, 1]\n        df_seq['linear_acc_z'] = linear_accel_seq_arr[:, 2]\n    \n    df_seq['linear_acc_mag'] = np.sqrt(df_seq['linear_acc_x']**2 + df_seq['linear_acc_y']**2 + df_seq['linear_acc_z']**2)\n    df_seq['linear_acc_mag_jerk'] = df_seq['linear_acc_mag'].diff().fillna(0)\n\n    # FFT frequency analysis\n\n    # 更新频率特征\n    fft_results = FFTforLinearAccel(df_seq[['linear_acc_x', 'linear_acc_y', 'linear_acc_z']].values)\n    amplitude_x = np.abs(fft_results[\"x\"])\n    amplitude_y = np.abs(fft_results['y'])\n    amplitude_z = np.abs(fft_results[\"z\"])\n\n    spectral_entropy_x, spectral_entropy_y, spectral_entropy_z = freqEntropyFromLinearAccelFFT(fft_results)\n\n    # 更新频率特征\n    df_seq['freq_entropy_x'] = spectral_entropy_x\n    df_seq['freq_entropy_y'] = spectral_entropy_y\n    df_seq['freq_entropy_z'] = spectral_entropy_z\n\n    # 计算频谱平坦度\n    spectral_flatness_x = np.exp(np.mean(np.log(amplitude_x + 1e-8))) / np.mean(amplitude_x)\n    spectral_flatness_y = np.exp(np.mean(np.log(amplitude_y + 1e-8))) / np.mean(amplitude_y)\n    spectral_flatness_z = np.exp(np.mean(np.log(amplitude_z + 1e-8))) / np.mean(amplitude_z)\n\n    # 更新频谱平坦度特征\n\n    '''\n    df_seq['spectral_flatness_x'] = spectral_flatness_x\n    df_seq['spectral_flatness_y'] = spectral_flatness_y\n    df_seq['spectral_flatness_z'] = spectral_flatness_z\n    '''\n    \n    \n\n    # 确保所有其他需要的特征也在 df_seq 中\n    df_seq_final_features = pd.DataFrame(index=df_seq.index)\n    for col_name in final_feature_cols:\n        if col_name in df_seq.columns:\n            df_seq_final_features[col_name] = df_seq[col_name]\n        else:\n            print(f\"CRITICAL ERROR IN PREDICT: Feature '{col_name}' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\")\n            df_seq_final_features[col_name] = 0\n    \n    # Calculate angular velocity from quaternions in predict function\n    if all(col in df_seq.columns for col in rot_cols_for_gravity_removal):\n        angular_vel_seq_arr = calculate_angular_velocity_from_quat(df_seq[rot_cols_for_gravity_removal])\n        df_seq['angular_vel_x'] = angular_vel_seq_arr[:, 0]\n        df_seq['angular_vel_y'] = angular_vel_seq_arr[:, 1]\n        df_seq['angular_vel_z'] = angular_vel_seq_arr[:, 2]\n    else:\n        print(f\"Warning: Missing quaternion columns for angular velocity calculation in predict. Filling with 0.\")\n        df_seq['angular_vel_x'] = 0\n        df_seq['angular_vel_y'] = 0\n        df_seq['angular_vel_z'] = 0\n\n    # Calculate angular distance from quaternions in predict function\n    if all(col in df_seq.columns for col in rot_cols_for_gravity_removal):\n        angular_dist_seq_arr = calculate_angular_distance(df_seq[rot_cols_for_gravity_removal])\n        df_seq['angular_distance'] = angular_dist_seq_arr\n    else:\n        print(f\"Warning: Missing quaternion columns for angular distance calculation in predict. Filling with 0.\")\n        df_seq['angular_distance'] = 0\n\n\n    for i in range(1, 6): \n        pixel_cols_tof = [f\"tof_{i}_v{p}\" for p in range(64)]\n        if not all(col in df_seq.columns for col in pixel_cols_tof):\n            print(f\"Warning: Missing some TOF pixel columns for tof_{i} in predict. Filling aggregates with 0.\")\n            df_seq[f'tof_{i}_mean'] = 0\n            df_seq[f'tof_{i}_std']  = 0\n            df_seq[f'tof_{i}_min']  = 0\n            df_seq[f'tof_{i}_max']  = 0\n            continue\n\n        tof_sensor_data = df_seq[pixel_cols_tof].replace(-1, np.nan)\n        df_seq[f'tof_{i}_mean'] = tof_sensor_data.mean(axis=1)\n        df_seq[f'tof_{i}_std']  = tof_sensor_data.std(axis=1)\n        df_seq[f'tof_{i}_min']  = tof_sensor_data.min(axis=1)\n        df_seq[f'tof_{i}_max']  = tof_sensor_data.max(axis=1)\n        \n    if 'tof_range_across_sensors' in final_feature_cols:\n        tof_mean_cols_for_contrast = [f'tof_{i}_mean' for i in range(1, 6) if f'tof_{i}_mean' in df_seq.columns]\n        thm_cols_for_contrast = [f'thm_{i}' for i in range(1, 6) if f'thm_{i}' in df_seq.columns]\n\n        if tof_mean_cols_for_contrast:\n            tof_values_for_contrast = df_seq[tof_mean_cols_for_contrast]\n            df_seq['tof_range_across_sensors'] = tof_values_for_contrast.max(axis=1) - tof_values_for_contrast.min(axis=1)\n            df_seq['tof_std_across_sensors'] = tof_values_for_contrast.std(axis=1)\n        else:\n            df_seq['tof_range_across_sensors'] = 0\n            df_seq['tof_std_across_sensors'] = 0\n\n        if thm_cols_for_contrast:\n            thm_values_for_contrast = df_seq[thm_cols_for_contrast]\n            df_seq['thm_range_across_sensors'] = thm_values_for_contrast.max(axis=1) - thm_values_for_contrast.min(axis=1)\n            df_seq['thm_std_across_sensors'] = thm_values_for_contrast.std(axis=1)\n        else:\n            df_seq['thm_range_across_sensors'] = 0\n            df_seq['thm_std_across_sensors'] = 0\n        \n    df_seq_final_features = pd.DataFrame(index=df_seq.index)\n    for col_name in final_feature_cols:\n        if col_name in df_seq.columns:\n            df_seq_final_features[col_name] = df_seq[col_name]\n        else:\n            print(f\"CRITICAL ERROR IN PREDICT: Feature '{col_name}' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\")\n            df_seq_final_features[col_name] = 0 \n            \n    mat_unscaled = df_seq_final_features.ffill().bfill().fillna(0).values.astype('float32')\n    \n    mat_scaled = scaler.transform(mat_unscaled)\n    \n    pad_input = pad_sequences([mat_scaled], maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n    \n    # ---------------------------------------------- #\n    # Blending Models\n    # ---------------------------------------------- #\n    predictions = []\n    for model in models:\n        idx = int(model.predict(pad_input, verbose=0).argmax(1)[0])\n        predictions.append(idx)\n    \n    idx = max(set(predictions), key=predictions.count)\n    return str(gesture_classes[idx])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T11:10:30.171791Z","iopub.execute_input":"2025-07-06T11:10:30.172474Z","iopub.status.idle":"2025-07-06T11:10:30.199574Z","shell.execute_reply.started":"2025-07-06T11:10:30.172448Z","shell.execute_reply":"2025-07-06T11:10:30.198877Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### 》》》**Submit Inference server**","metadata":{}},{"cell_type":"code","source":"import kaggle_evaluation.cmi_inference_server\ninference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        data_paths=(\n            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv',\n            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv',\n        )\n    )","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-07-06T11:10:30.200367Z","iopub.execute_input":"2025-07-06T11:10:30.200679Z","iopub.status.idle":"2025-07-06T11:10:39.613132Z","shell.execute_reply.started":"2025-07-06T11:10:30.200652Z","shell.execute_reply":"2025-07-06T11:10:39.612289Z"}},"outputs":[{"name":"stdout","text":"CRITICAL ERROR IN PREDICT: Feature 'angular_vel_x' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'angular_vel_y' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'angular_vel_z' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'angular_distance' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_1_mean' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_1_std' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_1_min' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_1_max' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_2_mean' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_2_std' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_2_min' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_2_max' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_3_mean' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_3_std' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_3_min' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_3_max' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_4_mean' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_4_std' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_4_min' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_4_max' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_5_mean' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_5_std' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_5_min' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_5_max' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n","output_type":"stream"},{"name":"stderr","text":"2025-07-06 11:10:31.558426: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\nI0000 00:00:1751800232.488803      79 cuda_dnn.cc:529] Loaded cuDNN version 90300\n2025-07-06 11:10:36.571168: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n","output_type":"stream"},{"name":"stdout","text":"CRITICAL ERROR IN PREDICT: Feature 'angular_vel_x' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'angular_vel_y' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'angular_vel_z' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'angular_distance' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_1_mean' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_1_std' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_1_min' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_1_max' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_2_mean' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_2_std' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_2_min' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_2_max' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_3_mean' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_3_std' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_3_min' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_3_max' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_4_mean' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_4_std' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_4_min' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_4_max' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_5_mean' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_5_std' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_5_min' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\nCRITICAL ERROR IN PREDICT: Feature 'tof_5_max' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"**测试和评估**","metadata":{}}]}