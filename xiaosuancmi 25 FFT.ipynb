{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2c2509a",
   "metadata": {
    "papermill": {
     "duration": 0.005669,
     "end_time": "2025-07-06T12:44:50.004143",
     "exception": false,
     "start_time": "2025-07-06T12:44:49.998474",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "OnlyInfKernel\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "590de8e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T12:44:50.014857Z",
     "iopub.status.busy": "2025-07-06T12:44:50.014043Z",
     "iopub.status.idle": "2025-07-06T12:44:50.021199Z",
     "shell.execute_reply": "2025-07-06T12:44:50.020406Z"
    },
    "papermill": {
     "duration": 0.013566,
     "end_time": "2025-07-06T12:44:50.022438",
     "exception": false,
     "start_time": "2025-07-06T12:44:50.008872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello wrld\n"
     ]
    }
   ],
   "source": [
    "print(\"hello wrld\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a828563",
   "metadata": {
    "papermill": {
     "duration": 0.004396,
     "end_time": "2025-07-06T12:44:50.031439",
     "exception": false,
     "start_time": "2025-07-06T12:44:50.027043",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **ℹ️INFO**\n",
    "* First, we would like to thank our participants for sharing their excellent baselines.\n",
    "    * [Two‑Branch Human‑Activity‑Recognition Pipeline (IMU + Thermopile/TOF  + SE‑CNN + BiLSTM + Attentio](https://www.kaggle.com/code/vonmainstein/imu-tof)\n",
    "\n",
    "### **ℹ️IMU+THM/TOF Great Related works(Published)**\n",
    "* Thanks for sharing your implementation of the new feature.\n",
    "    * LB.75 [CMI25 | IMU+THM/TOF |TF BiLSTM+GRU+Attention|LB.75](https://www.kaggle.com/code/hideyukizushi/cmi25-imu-thm-tof-tf-bilstm-gru-attention-lb-75)\n",
    "    * LB.76 [IMU Signal Processing Optimization](https://www.kaggle.com/code/rktqwe/lb-0-76-imu-thm-tof-tf-bilstm-gru-attention)\n",
    "    * LB.77 [Gravity Component Removal from Accelerometer Data](https://www.kaggle.com/code/rktqwe/lb-0-77-linear-accel-tf-bilstm-gru-attention)\n",
    "    * LB.78 [New Feature: Angular Velocity from Quaternion Derivatives](https://www.kaggle.com/code/nksusth/lb-0-78-quaternions-tf-bilstm-gru-attention)\n",
    "\n",
    "---\n",
    "\n",
    "### **ℹ️MyUpdate**\n",
    "* In previous notebooks that used \"IMU+TOF\" as features, the validation strategy was TTS, which raised concerns about its robustness.\n",
    "For this reason, we will release an implementation that simply blends five models and a model with a high ValidationScore.\n",
    "\n",
    "```\n",
    "predictions = []\n",
    "for model in models:\n",
    "    idx = int(model.predict(pad_input, verbose=0).argmax(1)[0])\n",
    "    predictions.append(idx)\n",
    "\n",
    "idx = max(set(predictions), key=predictions.count)\n",
    "return str(gesture_classes[idx])\n",
    "```\n",
    "\n",
    "* The validationScore for the local training  model included in this notebook is\n",
    "    * [ModelWeight](https://www.kaggle.com/datasets/hideyukizushi/20250627-cmi-b-102-b-105)\n",
    "\n",
    "```\n",
    "0.891134700273056\n",
    "0.8912659261884439\n",
    "0.8914825129445727\n",
    "0.8915471835009202\n",
    "0.8922128108549205\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7921c691",
   "metadata": {
    "papermill": {
     "duration": 0.004377,
     "end_time": "2025-07-06T12:44:50.040422",
     "exception": false,
     "start_time": "2025-07-06T12:44:50.036045",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "Model Detail\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4564eaa8",
   "metadata": {
    "papermill": {
     "duration": 0.005112,
     "end_time": "2025-07-06T12:44:50.050442",
     "exception": false,
     "start_time": "2025-07-06T12:44:50.045330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "InfPipeline\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97ff20d",
   "metadata": {
    "papermill": {
     "duration": 0.004605,
     "end_time": "2025-07-06T12:44:50.059740",
     "exception": false,
     "start_time": "2025-07-06T12:44:50.055135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 》》》**Importing the necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb70e2f7",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-07-06T12:44:50.069595Z",
     "iopub.status.busy": "2025-07-06T12:44:50.069402Z",
     "iopub.status.idle": "2025-07-06T12:45:04.856634Z",
     "shell.execute_reply": "2025-07-06T12:45:04.856008Z"
    },
    "papermill": {
     "duration": 14.793789,
     "end_time": "2025-07-06T12:45:04.858027",
     "exception": false,
     "start_time": "2025-07-06T12:44:50.064238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 12:44:53.722837: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751805893.879491      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751805893.933875      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os, json, joblib, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tensorflow.keras.utils import Sequence, to_categorical, pad_sequences\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, BatchNormalization, Activation, add, MaxPooling1D, Dropout,\n",
    "    Bidirectional, LSTM, GlobalAveragePooling1D, Dense, Multiply, Reshape,\n",
    "    Lambda, Concatenate, GRU, GaussianNoise\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdcfea9",
   "metadata": {
    "papermill": {
     "duration": 0.004603,
     "end_time": "2025-07-06T12:45:04.867700",
     "exception": false,
     "start_time": "2025-07-06T12:45:04.863097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 》》》**Fix Seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40615395",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T12:45:04.877855Z",
     "iopub.status.busy": "2025-07-06T12:45:04.877457Z",
     "iopub.status.idle": "2025-07-06T12:45:04.881855Z",
     "shell.execute_reply": "2025-07-06T12:45:04.881323Z"
    },
    "papermill": {
     "duration": 0.010676,
     "end_time": "2025-07-06T12:45:04.882959",
     "exception": false,
     "start_time": "2025-07-06T12:45:04.872283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.experimental.numpy.random.seed(seed)\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869df977",
   "metadata": {
    "papermill": {
     "duration": 0.00452,
     "end_time": "2025-07-06T12:45:04.892599",
     "exception": false,
     "start_time": "2025-07-06T12:45:04.888079",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 》》》**Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d77f7615",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T12:45:04.903167Z",
     "iopub.status.busy": "2025-07-06T12:45:04.902495Z",
     "iopub.status.idle": "2025-07-06T12:45:04.907466Z",
     "shell.execute_reply": "2025-07-06T12:45:04.906707Z"
    },
    "papermill": {
     "duration": 0.011358,
     "end_time": "2025-07-06T12:45:04.908589",
     "exception": false,
     "start_time": "2025-07-06T12:45:04.897231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ imports ready · tensorflow 2.18.0\n"
     ]
    }
   ],
   "source": [
    "# (Competition metric will only be imported when TRAINing)\n",
    "TRAIN = False                     # ← set to True when you want to train\n",
    "RAW_DIR = Path(\"/kaggle/input/cmi-detect-behavior-with-sensor-data\")\n",
    "PRETRAINED_DIR = Path(\"/kaggle/input/lb-0-78-quaternions-tf-bilstm-gru-attention\")  # used when TRAIN=False\n",
    "EXPORT_DIR = Path(\"./\")                                    # artefacts will be saved here\n",
    "BATCH_SIZE = 64\n",
    "PAD_PERCENTILE = 95\n",
    "LR_INIT = 5e-4\n",
    "WD = 3e-3\n",
    "MIXUP_ALPHA = 0.4\n",
    "EPOCHS = 160\n",
    "PATIENCE = 40\n",
    "\n",
    "\n",
    "print(\"▶ imports ready · tensorflow\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc1149e",
   "metadata": {
    "papermill": {
     "duration": 0.004558,
     "end_time": "2025-07-06T12:45:04.917796",
     "exception": false,
     "start_time": "2025-07-06T12:45:04.913238",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 》》》**Utility Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6368df22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T12:45:04.928099Z",
     "iopub.status.busy": "2025-07-06T12:45:04.927889Z",
     "iopub.status.idle": "2025-07-06T12:45:04.934757Z",
     "shell.execute_reply": "2025-07-06T12:45:04.934248Z"
    },
    "papermill": {
     "duration": 0.013057,
     "end_time": "2025-07-06T12:45:04.935772",
     "exception": false,
     "start_time": "2025-07-06T12:45:04.922715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Tensor Manipulations\n",
    "def time_sum(x):\n",
    "    return K.sum(x, axis=1)\n",
    "\n",
    "def squeeze_last_axis(x):\n",
    "    return tf.squeeze(x, axis=-1)\n",
    "\n",
    "def expand_last_axis(x):\n",
    "    return tf.expand_dims(x, axis=-1)\n",
    "\n",
    "def se_block(x, reduction=8):\n",
    "    ch = x.shape[-1]\n",
    "    se = GlobalAveragePooling1D()(x)\n",
    "    se = Dense(ch // reduction, activation='relu')(se)\n",
    "    se = Dense(ch, activation='sigmoid')(se)\n",
    "    se = Reshape((1, ch))(se)\n",
    "    return Multiply()([x, se])\n",
    "\n",
    "# Residual CNN Block with SE\n",
    "def residual_se_cnn_block(x, filters, kernel_size, pool_size=2, drop=0.3, wd=1e-4):\n",
    "    shortcut = x\n",
    "    for _ in range(2):\n",
    "        x = Conv1D(filters, kernel_size, padding='same', use_bias=False,\n",
    "                   kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "    x = se_block(x)\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, 1, padding='same', use_bias=False,\n",
    "                          kernel_regularizer=l2(wd))(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    x = add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(pool_size)(x)\n",
    "    x = Dropout(drop)(x)\n",
    "    return x\n",
    "\n",
    "def attention_layer(inputs):\n",
    "    score = Dense(1, activation='tanh')(inputs)\n",
    "    score = Lambda(squeeze_last_axis)(score)\n",
    "    weights = Activation('softmax')(score)\n",
    "    weights = Lambda(expand_last_axis)(weights)\n",
    "    context = Multiply()([inputs, weights])\n",
    "    context = Lambda(time_sum)(context)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2868c78d",
   "metadata": {
    "papermill": {
     "duration": 0.004519,
     "end_time": "2025-07-06T12:45:04.944948",
     "exception": false,
     "start_time": "2025-07-06T12:45:04.940429",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 》》》**Data Helpers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19617808",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T12:45:04.955186Z",
     "iopub.status.busy": "2025-07-06T12:45:04.954766Z",
     "iopub.status.idle": "2025-07-06T12:45:04.960740Z",
     "shell.execute_reply": "2025-07-06T12:45:04.960236Z"
    },
    "papermill": {
     "duration": 0.012156,
     "end_time": "2025-07-06T12:45:04.961697",
     "exception": false,
     "start_time": "2025-07-06T12:45:04.949541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Normalizes and cleans the time series sequence. \n",
    "\n",
    "def preprocess_sequence(df_seq: pd.DataFrame, feature_cols: list[str], scaler: StandardScaler):\n",
    "    mat = df_seq[feature_cols].ffill().bfill().fillna(0).values\n",
    "    return scaler.transform(mat).astype('float32')\n",
    "\n",
    "# MixUp the data argumentation in order to regularize the neural network. \n",
    "\n",
    "class MixupGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size, alpha=0.2):\n",
    "        self.X, self.y = X, y\n",
    "        self.batch = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.indices = np.arange(len(X))\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch))\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.indices[i*self.batch:(i+1)*self.batch]\n",
    "        Xb, yb = self.X[idx], self.y[idx]\n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        perm = np.random.permutation(len(Xb))\n",
    "        X_mix = lam * Xb + (1-lam) * Xb[perm]\n",
    "        y_mix = lam * yb + (1-lam) * yb[perm]\n",
    "        return X_mix, y_mix\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "652cbb81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T12:45:04.971991Z",
     "iopub.status.busy": "2025-07-06T12:45:04.971495Z",
     "iopub.status.idle": "2025-07-06T12:45:04.980193Z",
     "shell.execute_reply": "2025-07-06T12:45:04.979482Z"
    },
    "papermill": {
     "duration": 0.014827,
     "end_time": "2025-07-06T12:45:04.981220",
     "exception": false,
     "start_time": "2025-07-06T12:45:04.966393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_gravity_from_acc(acc_data, rot_data):\n",
    "\n",
    "    if isinstance(acc_data, pd.DataFrame):\n",
    "        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    else:\n",
    "        acc_values = acc_data\n",
    "\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = acc_values.shape[0]\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    \n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
    "            linear_accel[i, :] = acc_values[i, :] \n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except ValueError:\n",
    "             linear_accel[i, :] = acc_values[i, :]\n",
    "             \n",
    "    return linear_accel\n",
    "\n",
    "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200): # Assuming 200Hz sampling rate\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_vel = np.zeros((num_samples, 3))\n",
    "\n",
    "    for i in range(num_samples - 1):\n",
    "        q_t = quat_values[i]\n",
    "        q_t_plus_dt = quat_values[i+1]\n",
    "\n",
    "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
    "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "\n",
    "            # Calculate the relative rotation\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            \n",
    "            # Convert delta rotation to angular velocity vector\n",
    "            # The rotation vector (Euler axis * angle) scaled by 1/dt\n",
    "            # is a good approximation for small delta_rot\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except ValueError:\n",
    "            # If quaternion is invalid, angular velocity remains zero\n",
    "            pass\n",
    "            \n",
    "    return angular_vel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6482d2d",
   "metadata": {
    "papermill": {
     "duration": 0.004471,
     "end_time": "2025-07-06T12:45:04.990294",
     "exception": false,
     "start_time": "2025-07-06T12:45:04.985823",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## *v20: added FFT and freq analysis to feature engineering*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03eef2b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T12:45:05.000322Z",
     "iopub.status.busy": "2025-07-06T12:45:05.000064Z",
     "iopub.status.idle": "2025-07-06T12:45:05.009719Z",
     "shell.execute_reply": "2025-07-06T12:45:05.009168Z"
    },
    "papermill": {
     "duration": 0.015935,
     "end_time": "2025-07-06T12:45:05.010793",
     "exception": false,
     "start_time": "2025-07-06T12:45:04.994858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# 测试：生成一组 dummy 数据并进行验证\\nnp.random.seed(42)  # 保证结果可复现\\ndummy_data = np.random.randn(1000, 3)  # 生成 1000 个随机数据点，模拟加速度（x, y, z）\\n# 对加速度数据进行傅里叶变换并计算频域熵\\nfft_results = FFTforLinearAccel(dummy_data)\\nspectral_entropy = freqEntropyFromLinearAccelFFT(fft_results)\\n\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设 fs 是采样率\n",
    "fs = 100  # 设置采样率为 100Hz，当然根据你的数据来调整\n",
    "\n",
    "def FFTforLinearAccel(linear_accel):\n",
    "    \"\"\"\n",
    "    对加速度的 x, y, z 分量分别进行傅里叶变换，并返回各个分量的 FFT 结果。\n",
    "    \n",
    "    参数:\n",
    "    - linear_accel: 三维加速度数据，形状为 (n_samples, 3)，每一列对应 x, y, z 方向的加速度。\n",
    "    \n",
    "    返回:\n",
    "    - fft_results: 包含 x, y, z 分量 FFT 结果的字典。\n",
    "    \"\"\"\n",
    "    accelX = linear_accel[:, 0]  # x方向的加速度\n",
    "    accelY = linear_accel[:, 1]  # y方向的加速度\n",
    "    accelZ = linear_accel[:, 2]  # z方向的加速度\n",
    "    \n",
    "    fft_resultX = np.fft.fft(accelX)  # 对加速度数据做傅里叶变换\n",
    "    freqsX = np.fft.fftfreq(len(accelX), d=1/fs)  # 计算对应的频率轴\n",
    "\n",
    "    fft_resultY = np.fft.fft(accelY)  # y方向的傅里叶变换\n",
    "    freqsY = np.fft.fftfreq(len(accelY), d=1/fs)\n",
    "\n",
    "    fft_resultZ = np.fft.fft(accelZ)  # z方向的傅里叶变换\n",
    "    freqsZ = np.fft.fftfreq(len(accelZ), d=1/fs)\n",
    "\n",
    "    fft_results = {\n",
    "        'x': (fft_resultX, freqsX),\n",
    "        'y': (fft_resultY, freqsY),\n",
    "        'z': (fft_resultZ, freqsZ)\n",
    "    }\n",
    "    return fft_results\n",
    "\n",
    "\n",
    "def freqEntropyFromLinearAccelFFT(fft_results):\n",
    "    \"\"\"\n",
    "    计算从傅里叶变换结果得到的频域熵。\n",
    "    \n",
    "    参数:\n",
    "    - fft_results: 包含傅里叶变换结果和频率轴的字典。\n",
    "    \n",
    "    返回:\n",
    "    - spectral_entropy: 频域熵值。\n",
    "    \"\"\"\n",
    "    # 计算幅度\n",
    "    amplitude_x = np.abs(fft_results['x'][0])\n",
    "    amplitude_y = np.abs(fft_results['y'][0])\n",
    "    amplitude_z = np.abs(fft_results['z'][0])\n",
    "\n",
    "    # 归一化幅度\n",
    "    amplitude_x_norm = amplitude_x / np.sum(amplitude_x)\n",
    "    amplitude_y_norm = amplitude_y / np.sum(amplitude_y)\n",
    "    amplitude_z_norm = amplitude_z / np.sum(amplitude_z)\n",
    "    \n",
    "    # 计算频域熵\n",
    "    spectral_entropy_x = entropy(amplitude_x_norm)\n",
    "    spectral_entropy_y = entropy(amplitude_y_norm)\n",
    "    spectral_entropy_z = entropy(amplitude_z_norm)\n",
    "    \n",
    "    print(f\"Spectral Entropy for X: {spectral_entropy_x}\")\n",
    "    print(f\"Spectral Entropy for Y: {spectral_entropy_y}\")\n",
    "    print(f\"Spectral Entropy for Z: {spectral_entropy_z}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    return spectral_entropy_x, spectral_entropy_y, spectral_entropy_z\n",
    "'''\n",
    "\n",
    "# 测试：生成一组 dummy 数据并进行验证\n",
    "np.random.seed(42)  # 保证结果可复现\n",
    "dummy_data = np.random.randn(1000, 3)  # 生成 1000 个随机数据点，模拟加速度（x, y, z）\n",
    "# 对加速度数据进行傅里叶变换并计算频域熵\n",
    "fft_results = FFTforLinearAccel(dummy_data)\n",
    "spectral_entropy = freqEntropyFromLinearAccelFFT(fft_results)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faf98522",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T12:45:05.021086Z",
     "iopub.status.busy": "2025-07-06T12:45:05.020886Z",
     "iopub.status.idle": "2025-07-06T12:45:05.026302Z",
     "shell.execute_reply": "2025-07-06T12:45:05.025617Z"
    },
    "papermill": {
     "duration": 0.011715,
     "end_time": "2025-07-06T12:45:05.027349",
     "exception": false,
     "start_time": "2025-07-06T12:45:05.015634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_angular_distance(rot_data):\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_dist = np.zeros(num_samples)\n",
    "\n",
    "    for i in range(num_samples - 1):\n",
    "        q1 = quat_values[i]\n",
    "        q2 = quat_values[i+1]\n",
    "\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n",
    "           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
    "            angular_dist[i] = 0 # Or np.nan, depending on desired behavior\n",
    "            continue\n",
    "        try:\n",
    "            # Conversion of quaternions to Rotation objects\n",
    "            r1 = R.from_quat(q1)\n",
    "            r2 = R.from_quat(q2)\n",
    "\n",
    "            # Calculating angular distance: 2 * arccos(|real(p * q*)|)\n",
    "            # where p* is the conjugate quaternion of q\n",
    "            # In scipy.spatial.transform.Rotation, r1.inv() * r2 gives the relative rotation.\n",
    "            # The angle of this relative rotation is the angular distance.\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            \n",
    "            # The angle of the rotation vector corresponds to the angular distance\n",
    "            # The norm of the rotation vector is the angle in radians\n",
    "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "            angular_dist[i] = angle\n",
    "        except ValueError:\n",
    "            angular_dist[i] = 0 # In case of invalid quaternions\n",
    "            pass\n",
    "            \n",
    "    return angular_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d93fd7e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T12:45:05.037757Z",
     "iopub.status.busy": "2025-07-06T12:45:05.037585Z",
     "iopub.status.idle": "2025-07-06T12:45:05.045020Z",
     "shell.execute_reply": "2025-07-06T12:45:05.044482Z"
    },
    "papermill": {
     "duration": 0.01393,
     "end_time": "2025-07-06T12:45:05.045993",
     "exception": false,
     "start_time": "2025-07-06T12:45:05.032063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 例子\\nlinear_accel = np.array([0.1, 0.2, 0.3])  # 三维加速度向量\\norientation = \\'seated lean\\'\\n\\nadjusted_accel = equivalentToSittingStraight(linear_accel, orientation)\\nprint(f\"Adjusted Linear Acceleration: {adjusted_accel}\")\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import numpy as np\n",
    "\n",
    "def equivalentToSittingStraight(linear_accel, orientation):\n",
    "    \"\"\"\n",
    "    将加速度数据转换为坐直（sitting straight）姿势下的加速度。\n",
    "    \n",
    "    参数：\n",
    "    - linear_accel: 三维加速度数据（如经过重力去除后的平动加速度）。\n",
    "    - orientation: 当前的姿势信息（如 \"seated lean\", \"seated straight\", \"lie on side\" 等）。\n",
    "    \n",
    "    返回：\n",
    "    - 转换为坐正姿势后的加速度数据。\n",
    "    \"\"\"\n",
    "    # 检查 \"seated lean\" 姿势\n",
    "    if re.search(r'\\bseated\\s*lean\\b', orientation, re.IGNORECASE):\n",
    "        rotation = R.from_euler('xyz', [0, -30, 0], degrees=True)  # 负的 Pitch\n",
    "    # 检查 \"seated straight\" 姿势\n",
    "    elif re.search(r'\\bseated\\s*straight\\b', orientation, re.IGNORECASE):\n",
    "        rotation = R.from_euler('xyz', [0, 0, 0], degrees=True)  # 没有旋转\n",
    "    # 检查 \"lie on side\" 姿势\n",
    "    elif re.search(r'\\blie\\s*on\\s*side\\b', orientation, re.IGNORECASE):\n",
    "        rotation = R.from_euler('xyz', [90, 90, 0], degrees=True)  # 90° Roll 和 90° Pitch\n",
    "    # 检查 \"lie on back\" 姿势\n",
    "    elif re.search(r'\\blie\\s*on\\s*back\\b', orientation, re.IGNORECASE):\n",
    "        rotation = R.from_euler('xyz', [0, 90, 0], degrees=True)  # 90° Pitch\n",
    "    else:\n",
    "        # 如果姿势没有匹配，返回原始加速度\n",
    "        rotation = R.from_euler('xyz', [0, 0, 0], degrees=True)  # 默认无旋转\n",
    "    \n",
    "    # 获取旋转矩阵的逆\n",
    "    inverseRotation = rotation.inv().as_matrix()\n",
    "\n",
    "    # 确保 linear_accel 是三维向量（例如形状为 (3,) 或 (n, 3)）\n",
    "    # 如果 linear_accel 是一个 1D 数组，转换为 (3, ) 的 3D 向量\n",
    "    if linear_accel.ndim == 1:\n",
    "        linear_accel = linear_accel.reshape(1, 3)  # 将其转为 1 x 3 数组\n",
    "\n",
    "    # 使用逆旋转矩阵对加速度进行转换\n",
    "    adjusted_accel = inverseRotation @ linear_accel.T  # 矩阵乘法，确保形状匹配\n",
    "\n",
    "    return adjusted_accel.T  # 返回转换后的加速度数据\n",
    "\n",
    "'''\n",
    "# 例子\n",
    "linear_accel = np.array([0.1, 0.2, 0.3])  # 三维加速度向量\n",
    "orientation = 'seated lean'\n",
    "\n",
    "adjusted_accel = equivalentToSittingStraight(linear_accel, orientation)\n",
    "print(f\"Adjusted Linear Acceleration: {adjusted_accel}\")\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8825e5",
   "metadata": {
    "papermill": {
     "duration": 0.004632,
     "end_time": "2025-07-06T12:45:05.055355",
     "exception": false,
     "start_time": "2025-07-06T12:45:05.050723",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 》》》**Model Definition - Two Branch Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "128bd67a",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-07-06T12:45:05.065820Z",
     "iopub.status.busy": "2025-07-06T12:45:05.065645Z",
     "iopub.status.idle": "2025-07-06T12:45:07.797837Z",
     "shell.execute_reply": "2025-07-06T12:45:07.797159Z"
    },
    "papermill": {
     "duration": 2.738793,
     "end_time": "2025-07-06T12:45:07.799169",
     "exception": false,
     "start_time": "2025-07-06T12:45:05.060376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751805905.961729      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "def build_two_branch_model(pad_len, imu_dim, tof_dim, n_classes, wd=1e-4):\n",
    "    inp = Input(shape=(pad_len, imu_dim+tof_dim))\n",
    "    imu = Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "    tof = Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "\n",
    "    # IMU deep branch\n",
    "    x1 = residual_se_cnn_block(imu, 64, 3, drop=0.1, wd=wd)\n",
    "    x1 = residual_se_cnn_block(x1, 128, 5, drop=0.1, wd=wd)\n",
    "\n",
    "    # TOF/Thermal lighter branch\n",
    "    x2 = Conv1D(64, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(tof)\n",
    "    x2 = BatchNormalization()(x2); x2 = Activation('relu')(x2)\n",
    "    x2 = MaxPooling1D(2)(x2); x2 = Dropout(0.2)(x2)\n",
    "    x2 = Conv1D(128, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x2)\n",
    "    x2 = BatchNormalization()(x2); x2 = Activation('relu')(x2)\n",
    "    x2 = MaxPooling1D(2)(x2); x2 = Dropout(0.2)(x2)\n",
    "\n",
    "    merged = Concatenate()([x1, x2])\n",
    "\n",
    "    xa = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n",
    "    xb = Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n",
    "    xc = GaussianNoise(0.09)(merged)\n",
    "    xc = Dense(16, activation='elu')(xc)\n",
    "    \n",
    "    x = Concatenate()([xa, xb, xc])\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = attention_layer(x)\n",
    "\n",
    "    for units, drop in [(256, 0.5), (128, 0.3)]:\n",
    "        x = Dense(units, use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x); x = Activation('relu')(x)\n",
    "        x = Dropout(drop)(x)\n",
    "\n",
    "    out = Dense(n_classes, activation='softmax', kernel_regularizer=l2(wd))(x)\n",
    "    return Model(inp, out)\n",
    "\n",
    "tmp_model = build_two_branch_model(127,7,325,18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c46778",
   "metadata": {
    "papermill": {
     "duration": 0.004864,
     "end_time": "2025-07-06T12:45:07.809303",
     "exception": false,
     "start_time": "2025-07-06T12:45:07.804439",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 》》》**Training / Inference Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07d72d92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T12:45:07.820362Z",
     "iopub.status.busy": "2025-07-06T12:45:07.819873Z",
     "iopub.status.idle": "2025-07-06T12:45:07.824069Z",
     "shell.execute_reply": "2025-07-06T12:45:07.823362Z"
    },
    "papermill": {
     "duration": 0.010988,
     "end_time": "2025-07-06T12:45:07.825166",
     "exception": false,
     "start_time": "2025-07-06T12:45:07.814178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world custom objects\n"
     ]
    }
   ],
   "source": [
    "custom_objs = {\n",
    "    'time_sum': time_sum,\n",
    "    'squeeze_last_axis': squeeze_last_axis,\n",
    "    'expand_last_axis': expand_last_axis,\n",
    "    'se_block': se_block,\n",
    "    'residual_se_cnn_block': residual_se_cnn_block,\n",
    "    'attention_layer': attention_layer,\n",
    "}\n",
    "\n",
    "print(\"hello world custom objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb84fc4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T12:45:07.836440Z",
     "iopub.status.busy": "2025-07-06T12:45:07.836211Z",
     "iopub.status.idle": "2025-07-06T12:45:08.420819Z",
     "shell.execute_reply": "2025-07-06T12:45:08.420049Z"
    },
    "papermill": {
     "duration": 0.591794,
     "end_time": "2025-07-06T12:45:08.421920",
     "exception": false,
     "start_time": "2025-07-06T12:45:07.830126",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ INFERENCE MODE – loading artefacts from /kaggle/input/lb-0-78-quaternions-tf-bilstm-gru-attention\n",
      "  Model, scaler, feature_cols, pad_len loaded – ready for evaluation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if TRAIN:\n",
    "    print(\"▶ TRAIN MODE – loading dataset …\")\n",
    "    df = pd.read_csv(RAW_DIR / \"train.csv\")\n",
    "\n",
    "    train_dem_df = pd.read_csv(RAW_DIR / \"train_demographics.csv\")\n",
    "    df_for_groups = pd.merge(df.copy(), train_dem_df, on='subject', how='left')\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    df['gesture_int'] = le.fit_transform(df['gesture'])\n",
    "    np.save(EXPORT_DIR / \"gesture_classes.npy\", le.classes_)\n",
    "    gesture_classes = le.classes_\n",
    "\n",
    "    print(\"  Calculating base engineered IMU features (magnitude, angle)...\")\n",
    "    df['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n",
    "    df['rot_angle'] = 2 * np.arccos(df['rot_w'].clip(-1, 1))\n",
    "    \n",
    "    print(\"  Calculating engineered IMU derivatives (jerk, angular velocity) for original acc_mag...\")\n",
    "    df['acc_mag_jerk'] = df.groupby('sequence_id')['acc_mag'].diff().fillna(0)\n",
    "    df['rot_angle_vel'] = df.groupby('sequence_id')['rot_angle'].diff().fillna(0)\n",
    "\n",
    "    print(\"  Removing gravity and calculating linear acceleration features...\")\n",
    "\n",
    "    linear_accel_list = []\n",
    "    \n",
    "    freq_entropy_x_list = []\n",
    "    freq_entropy_y_list = []\n",
    "    freq_entropy_z_list = []\n",
    "\n",
    "    spectral_flatness_x_list = []\n",
    "    spectral_flatness_y_list = []\n",
    "    spectral_flatness_z_list = []\n",
    "\n",
    "    for _, group in df.groupby('sequence_id'):\n",
    "        acc_data_group = group[['acc_x', 'acc_y', 'acc_z']]\n",
    "        rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "        pose = group['orientation'].iloc[0]\n",
    "        linear_accel_group = remove_gravity_from_acc(acc_data_group, rot_data_group)\n",
    "        #print(\"preTarget\")\n",
    "        linear_accel_group = equivalentToSittingStraight (linear_accel_group, pose)\n",
    "        #print(\"postTarget\")\n",
    "        linear_accel_list.append(pd.DataFrame(linear_accel_group, columns=['linear_acc_x', 'linear_acc_y', 'linear_acc_z'], index=group.index))\n",
    "\n",
    "        fft_results = FFTforLinearAccel(linear_accel_group)\n",
    "\n",
    "        freq_entropy_x, freq_entropy_y, freq_entropy_z = freqEntropyFromLinearAccelFFT(fft_results)\n",
    "    \n",
    "\n",
    "        freq_entropy_x_list.append(freq_entropy_x)\n",
    "        freq_entropy_y_list.append(freq_entropy_y)\n",
    "        freq_entropy_z_list.append(freq_entropy_z)\n",
    "    \n",
    "\n",
    "        print(\"FFT freq analysis of linear acceleration\")\n",
    "        \n",
    "        amplitude_x = np.abs(fft_results['x'][0])\n",
    "        amplitude_y = np.abs(fft_results['y'][0])\n",
    "        amplitude_z = np.abs(fft_results['z'][0])\n",
    "\n",
    "        \n",
    "        print(\"Freq entropy from lindear acceleration\")\n",
    "\n",
    "        print(\"Spectral flatness\")\n",
    "        spectral_flatness_x = np.exp(np.mean(np.log(amplitude_x + 1e-8))) / np.mean(amplitude_x)\n",
    "        spectral_flatness_y = np.exp(np.mean(np.log(amplitude_y + 1e-8))) / np.mean(amplitude_y)\n",
    "        spectral_flatness_z = np.exp(np.mean(np.log(amplitude_z + 1e-8))) / np.mean(amplitude_z)\n",
    "\n",
    "        \n",
    "        \n",
    "        spectral_flatness_x_list.append(spectral_flatness_x)\n",
    "        spectral_flatness_y_list.append(spectral_flatness_y)\n",
    "        spectral_flatness_z_list.append(spectral_flatness_z)\n",
    "\n",
    "\n",
    "    print(freq_entropy_x_list)\n",
    "    df_linear_accel = pd.concat(linear_accel_list)\n",
    "    df = pd.concat([df, df_linear_accel], axis=1)\n",
    "\n",
    "    print\n",
    "    df['freq_entropy_x'] = pd.concat([pd.Series(x) for x in freq_entropy_x_list], axis=0).reset_index(drop=True)\n",
    "    df['freq_entropy_y'] = pd.concat([pd.Series(x) for x in freq_entropy_y_list], axis=0).reset_index(drop=True)\n",
    "    df['freq_entropy_z'] = pd.concat([pd.Series(x) for x in freq_entropy_z_list], axis=0).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # Convert each float in the lists to a pandas Series before concatenating\n",
    "    '''\n",
    "    df['spectral_flatness_x'] = pd.concat([pd.Series([x]) for x in spectral_flatness_x_list], axis=0).reset_index(drop=True)\n",
    "    df['spectral_flatness_y'] = pd.concat([pd.Series([x]) for x in spectral_flatness_y_list], axis=0).reset_index(drop=True)\n",
    "    df['spectral_flatness_z'] = pd.concat([pd.Series([x]) for x in spectral_flatness_z_list], axis=0).reset_index(drop=True)\n",
    "\n",
    "    '''\n",
    "    \n",
    "\n",
    "    df['linear_acc_mag'] = np.sqrt(df['linear_acc_x']**2 + df['linear_acc_y']**2 + df['linear_acc_z']**2)\n",
    "    df['linear_acc_mag_jerk'] = df.groupby('sequence_id')['linear_acc_mag'].diff().fillna(0)\n",
    "\n",
    "    print(\"  Calculating angular velocity from quaternion derivatives...\")\n",
    "    angular_vel_list = []\n",
    "    for _, group in df.groupby('sequence_id'):\n",
    "        rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "        angular_vel_group = calculate_angular_velocity_from_quat(rot_data_group)\n",
    "        angular_vel_list.append(pd.DataFrame(angular_vel_group, columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'], index=group.index))\n",
    "    \n",
    "    df_angular_vel = pd.concat(angular_vel_list)\n",
    "    df = pd.concat([df, df_angular_vel], axis=1)\n",
    "\n",
    "    print(\"  Calculating angular distance between successive quaternions...\")\n",
    "    angular_distance_list = []\n",
    "    for _, group in df.groupby('sequence_id'):\n",
    "        rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "        angular_dist_group = calculate_angular_distance(rot_data_group)\n",
    "        angular_distance_list.append(pd.DataFrame(angular_dist_group, columns=['angular_distance'], index=group.index))\n",
    "    \n",
    "    df_angular_distance = pd.concat(angular_distance_list)\n",
    "    df = pd.concat([df, df_angular_distance], axis=1)\n",
    "\n",
    "    meta_cols = { } # This was an empty dict in your provided code, keeping it as is.\n",
    "\n",
    "    imu_cols_base = ['linear_acc_x', 'linear_acc_y', 'linear_acc_z']\n",
    "    imu_cols_base.extend([c for c in df.columns if c.startswith('rot_') and c not in ['rot_angle', 'rot_angle_vel']])\n",
    "    \n",
    "    imu_engineered_features = [\n",
    "        'acc_mag', 'rot_angle',\n",
    "        'acc_mag_jerk', 'rot_angle_vel',\n",
    "        'linear_acc_mag', 'linear_acc_mag_jerk',\n",
    "        'angular_vel_x', 'angular_vel_y', 'angular_vel_z', # Existing new features\n",
    "        'angular_distance', # Added new feature\n",
    "        'freq_entropy_x',# freq entropy\n",
    "        'freq_entropy_y',\n",
    "        'freq_entropy_z'\n",
    "\n",
    "        '''\n",
    "        'spectral_flatness_x',#freq flatness\n",
    "        'spectral_flatness_y',\n",
    "        'spectral_flatness_z'\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "    ]\n",
    "    imu_cols = imu_cols_base + imu_engineered_features\n",
    "    imu_cols = list(dict.fromkeys(imu_cols)) # Для удаления дубликатов\n",
    "\n",
    "    thm_cols_original = [c for c in df.columns if c.startswith('thm_')]\n",
    "    \n",
    "    tof_aggregated_cols_template = []\n",
    "    for i in range(1, 6):\n",
    "        tof_aggregated_cols_template.extend([f'tof_{i}_mean', f'tof_{i}_std', f'tof_{i}_min', f'tof_{i}_max'])\n",
    "\n",
    "    final_feature_cols = imu_cols + thm_cols_original + tof_aggregated_cols_template\n",
    "    imu_dim_final = len(imu_cols)\n",
    "    tof_thm_aggregated_dim_final = len(thm_cols_original) + len(tof_aggregated_cols_template)\n",
    "    \n",
    "    print(f\"  IMU (incl. engineered & derivatives) {imu_dim_final} | THM + Aggregated TOF {tof_thm_aggregated_dim_final} | total {len(final_feature_cols)} features\")\n",
    "    np.save(EXPORT_DIR / \"feature_cols.npy\", np.array(final_feature_cols))\n",
    "\n",
    "    print(\"  Building sequences with aggregated TOF and preparing data for scaler...\")\n",
    "    seq_gp = df.groupby('sequence_id') \n",
    "    \n",
    "    all_steps_for_scaler_list = []\n",
    "    X_list_unscaled, y_list_int_for_stratify, lens = [], [], [] \n",
    "\n",
    "    for seq_id, seq_df_orig in seq_gp:\n",
    "        seq_df = seq_df_orig.copy()\n",
    "\n",
    "        for i in range(1, 6):\n",
    "            pixel_cols_tof = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "            tof_sensor_data = seq_df[pixel_cols_tof].replace(-1, np.nan)\n",
    "            seq_df[f'tof_{i}_mean'] = tof_sensor_data.mean(axis=1)\n",
    "            seq_df[f'tof_{i}_std']  = tof_sensor_data.std(axis=1)\n",
    "            seq_df[f'tof_{i}_min']  = tof_sensor_data.min(axis=1)\n",
    "            seq_df[f'tof_{i}_max']  = tof_sensor_data.max(axis=1)\n",
    "        \n",
    "        mat_unscaled = seq_df[final_feature_cols].ffill().bfill().fillna(0).values.astype('float32')\n",
    "        \n",
    "        all_steps_for_scaler_list.append(mat_unscaled)\n",
    "        X_list_unscaled.append(mat_unscaled)\n",
    "        y_list_int_for_stratify.append(seq_df['gesture_int'].iloc[0])\n",
    "        lens.append(len(mat_unscaled))\n",
    "\n",
    "    print(\"  Fitting StandardScaler...\")\n",
    "    all_steps_concatenated = np.concatenate(all_steps_for_scaler_list, axis=0)\n",
    "    scaler = StandardScaler().fit(all_steps_concatenated)\n",
    "    joblib.dump(scaler, EXPORT_DIR / \"scaler.pkl\")\n",
    "    del all_steps_for_scaler_list, all_steps_concatenated\n",
    "\n",
    "    print(\"  Scaling and padding sequences...\")\n",
    "    X_scaled_list = [scaler.transform(x_seq) for x_seq in X_list_unscaled]\n",
    "    del X_list_unscaled\n",
    "\n",
    "    pad_len = int(np.percentile(lens, PAD_PERCENTILE))\n",
    "    np.save(EXPORT_DIR / \"sequence_maxlen.npy\", pad_len)\n",
    "    \n",
    "    X = pad_sequences(X_scaled_list, maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n",
    "    del X_scaled_list\n",
    "    \n",
    "    y_int_for_stratify = np.array(y_list_int_for_stratify)\n",
    "    y = to_categorical(y_int_for_stratify, num_classes=len(le.classes_))\n",
    "\n",
    "    print(\"  Splitting data and preparing for training...\")\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=82, stratify=y_int_for_stratify)\n",
    "\n",
    "    cw_vals = compute_class_weight('balanced', classes=np.arange(len(le.classes_)), y=y_int_for_stratify)\n",
    "    class_weight = dict(enumerate(cw_vals))\n",
    "\n",
    "    model = build_two_branch_model(pad_len, imu_dim_final, tof_thm_aggregated_dim_final, len(le.classes_), wd=WD)\n",
    "    \n",
    "    steps = len(X_tr) // BATCH_SIZE\n",
    "    lr_sched = tf.keras.optimizers.schedules.CosineDecayRestarts(5e-4, first_decay_steps=15 * steps) \n",
    "    \n",
    "    model.compile(optimizer=Adam(lr_sched),\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    train_gen = MixupGenerator(X_tr, y_tr, batch_size=BATCH_SIZE, alpha=MIXUP_ALPHA)\n",
    "    cb = EarlyStopping(patience=PATIENCE, restore_best_weights=True, verbose=1, monitor='val_accuracy', mode='max')\n",
    "    \n",
    "    print(\"  Starting model training...\")\n",
    "    model.fit(train_gen, epochs=EPOCHS, validation_data=(X_val, y_val),\n",
    "              class_weight=class_weight, callbacks=[cb], verbose=1)\n",
    "\n",
    "    model.save(EXPORT_DIR / \"gesture_two_branch_mixup.h5\")\n",
    "    print(\"✔ Training done – artefacts saved in\", EXPORT_DIR)\n",
    "\n",
    "    from cmi_2025_metric_copy_for_import import CompetitionMetric\n",
    "    preds_val = model.predict(X_val).argmax(1)\n",
    "    true_val_int  = y_val.argmax(1)\n",
    "    \n",
    "    h_f1 = CompetitionMetric().calculate_hierarchical_f1(\n",
    "        pd.DataFrame({'gesture': le.classes_[true_val_int]}),\n",
    "        pd.DataFrame({'gesture': le.classes_[preds_val]}))\n",
    "    print(\"Hold‑out H‑F1 =\", round(h_f1, 4))\n",
    "else:\n",
    "    print(\"▶ INFERENCE MODE – loading artefacts from\", PRETRAINED_DIR)\n",
    "    final_feature_cols = np.load(PRETRAINED_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n",
    "    pad_len        = int(np.load(PRETRAINED_DIR / \"sequence_maxlen.npy\"))\n",
    "    scaler         = joblib.load(PRETRAINED_DIR / \"scaler.pkl\")\n",
    "    gesture_classes = np.load(PRETRAINED_DIR / \"gesture_classes.npy\", allow_pickle=True)\n",
    "\n",
    "    # Re-calculate imu_dim_final based on the actual features that will be used\n",
    "    # Убедитесь, что 'angular_distance' учитывается здесь при инференсе\n",
    "    imu_features_in_final_cols = [c for c in final_feature_cols if any(c.startswith(prefix) for prefix in ['linear_acc_', 'acc_', 'rot_', 'angular_vel_', 'angular_distance'])]\n",
    "    imu_dim_final = len(imu_features_in_final_cols)\n",
    "\n",
    "    tof_thm_aggregated_dim_final = len(final_feature_cols) - imu_dim_final\n",
    "\n",
    "   \n",
    "    \n",
    "    model = load_model(PRETRAINED_DIR / \"gesture_two_branch_mixup.h5\",\n",
    "                       compile=False, custom_objects=custom_objs)\n",
    "    print(\"  Model, scaler, feature_cols, pad_len loaded – ready for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b36596",
   "metadata": {
    "papermill": {
     "duration": 0.005042,
     "end_time": "2025-07-06T12:45:08.432371",
     "exception": false,
     "start_time": "2025-07-06T12:45:08.427329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d2b764f",
   "metadata": {
    "papermill": {
     "duration": 0.004834,
     "end_time": "2025-07-06T12:45:08.442175",
     "exception": false,
     "start_time": "2025-07-06T12:45:08.437341",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "Blending Model\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6adf50d",
   "metadata": {
    "papermill": {
     "duration": 0.004781,
     "end_time": "2025-07-06T12:45:08.451920",
     "exception": false,
     "start_time": "2025-07-06T12:45:08.447139",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 》》》**LoadBlendingModels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae8e82b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T12:45:08.462715Z",
     "iopub.status.busy": "2025-07-06T12:45:08.462472Z",
     "iopub.status.idle": "2025-07-06T12:45:11.369290Z",
     "shell.execute_reply": "2025-07-06T12:45:11.368524Z"
    },
    "papermill": {
     "duration": 2.913911,
     "end_time": "2025-07-06T12:45:11.370713",
     "exception": false,
     "start_time": "2025-07-06T12:45:08.456802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "model = load_model(\"/kaggle/input/lb-0-78-quaternions-tf-bilstm-gru-attention/gesture_two_branch_mixup.h5\",compile=False, custom_objects=custom_objs)\n",
    "models.append(model)\n",
    "\n",
    "\n",
    "model = load_model(\"/kaggle/input/20250627-cmi-b-102-b-105/0.8914825129445727_.h5\",compile=False, custom_objects=custom_objs)\n",
    "models.append(model)\n",
    "model = load_model(\"/kaggle/input/20250627-cmi-b-102-b-105/0.8912659261884439_.h5\",compile=False, custom_objects=custom_objs)\n",
    "models.append(model)\n",
    "model = load_model(\"/kaggle/input/20250627-cmi-b-102-b-105/0.891134700273056_.h5\",compile=False, custom_objects=custom_objs)\n",
    "models.append(model)\n",
    "model = load_model(\"/kaggle/input/20250627-cmi-b-102-b-105/0.8915471835009202_.h5\",compile=False, custom_objects=custom_objs)\n",
    "models.append(model)\n",
    "model = load_model(\"/kaggle/input/20250627-cmi-b-102-b-105/0.8922128108549205_.h5\",compile=False, custom_objects=custom_objs)\n",
    "models.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd09c46",
   "metadata": {
    "papermill": {
     "duration": 0.004865,
     "end_time": "2025-07-06T12:45:11.381078",
     "exception": false,
     "start_time": "2025-07-06T12:45:11.376213",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 》》》**Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47efc148",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T12:45:11.392160Z",
     "iopub.status.busy": "2025-07-06T12:45:11.391895Z",
     "iopub.status.idle": "2025-07-06T12:45:11.408654Z",
     "shell.execute_reply": "2025-07-06T12:45:11.408130Z"
    },
    "papermill": {
     "duration": 0.02361,
     "end_time": "2025-07-06T12:45:11.409666",
     "exception": false,
     "start_time": "2025-07-06T12:45:11.386056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    df_seq = sequence.to_pandas()\n",
    "\n",
    "    df_seq['acc_mag'] = np.sqrt(df_seq['acc_x']**2 + df_seq['acc_y']**2 + df_seq['acc_z']**2)\n",
    "    df_seq['rot_angle'] = 2 * np.arccos(df_seq['rot_w'].clip(-1, 1))\n",
    "    df_seq['acc_mag_jerk'] = df_seq['acc_mag'].diff().fillna(0)\n",
    "    df_seq['rot_angle_vel'] = df_seq['rot_angle'].diff().fillna(0)\n",
    "\n",
    "    \n",
    "\n",
    "    acc_cols_for_gravity_removal = ['acc_x', 'acc_y', 'acc_z']\n",
    "    rot_cols_for_gravity_removal = ['rot_x', 'rot_y', 'rot_z', 'rot_w']\n",
    "\n",
    "    if not all(col in df_seq.columns for col in acc_cols_for_gravity_removal + rot_cols_for_gravity_removal):\n",
    "        print(f\"Warning: Missing raw acc/rot columns for gravity removal in predict for sequence. Using raw acc as linear.\")\n",
    "        df_seq['linear_acc_x'] = df_seq.get('acc_x', 0)\n",
    "        df_seq['linear_acc_y'] = df_seq.get('acc_y', 0)\n",
    "        df_seq['linear_acc_z'] = df_seq.get('acc_z', 0)\n",
    "    else:\n",
    "        acc_data_seq = df_seq[acc_cols_for_gravity_removal]\n",
    "        rot_data_seq = df_seq[rot_cols_for_gravity_removal]\n",
    "        linear_accel_seq_arr = remove_gravity_from_acc(acc_data_seq, rot_data_seq)\n",
    "        \n",
    "        df_seq['linear_acc_x'] = linear_accel_seq_arr[:, 0]\n",
    "        df_seq['linear_acc_y'] = linear_accel_seq_arr[:, 1]\n",
    "        df_seq['linear_acc_z'] = linear_accel_seq_arr[:, 2]\n",
    "    \n",
    "    df_seq['linear_acc_mag'] = np.sqrt(df_seq['linear_acc_x']**2 + df_seq['linear_acc_y']**2 + df_seq['linear_acc_z']**2)\n",
    "    df_seq['linear_acc_mag_jerk'] = df_seq['linear_acc_mag'].diff().fillna(0)\n",
    "\n",
    "    # FFT frequency analysis\n",
    "\n",
    "    # 更新频率特征\n",
    "    fft_results = FFTforLinearAccel(df_seq[['linear_acc_x', 'linear_acc_y', 'linear_acc_z']].values)\n",
    "    amplitude_x = np.abs(fft_results[\"x\"])\n",
    "    amplitude_y = np.abs(fft_results['y'])\n",
    "    amplitude_z = np.abs(fft_results[\"z\"])\n",
    "\n",
    "    spectral_entropy_x, spectral_entropy_y, spectral_entropy_z = freqEntropyFromLinearAccelFFT(fft_results)\n",
    "\n",
    "    # 更新频率特征\n",
    "    df_seq['freq_entropy_x'] = spectral_entropy_x\n",
    "    df_seq['freq_entropy_y'] = spectral_entropy_y\n",
    "    df_seq['freq_entropy_z'] = spectral_entropy_z\n",
    "\n",
    "    # 计算频谱平坦度\n",
    "    spectral_flatness_x = np.exp(np.mean(np.log(amplitude_x + 1e-8))) / np.mean(amplitude_x)\n",
    "    spectral_flatness_y = np.exp(np.mean(np.log(amplitude_y + 1e-8))) / np.mean(amplitude_y)\n",
    "    spectral_flatness_z = np.exp(np.mean(np.log(amplitude_z + 1e-8))) / np.mean(amplitude_z)\n",
    "\n",
    "    # 更新频谱平坦度特征\n",
    "    '''\n",
    "    df_seq['spectral_flatness_x'] = spectral_flatness_x\n",
    "    df_seq['spectral_flatness_y'] = spectral_flatness_y\n",
    "    df_seq['spectral_flatness_z'] = spectral_flatness_z\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # 确保所有其他需要的特征也在 df_seq 中\n",
    "    df_seq_final_features = pd.DataFrame(index=df_seq.index)\n",
    "    for col_name in final_feature_cols:\n",
    "        if col_name in df_seq.columns:\n",
    "            df_seq_final_features[col_name] = df_seq[col_name]\n",
    "        else:\n",
    "            print(f\"CRITICAL ERROR IN PREDICT: Feature '{col_name}' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\")\n",
    "            df_seq_final_features[col_name] = 0\n",
    "    \n",
    "    # Calculate angular velocity from quaternions in predict function\n",
    "    if all(col in df_seq.columns for col in rot_cols_for_gravity_removal):\n",
    "        angular_vel_seq_arr = calculate_angular_velocity_from_quat(df_seq[rot_cols_for_gravity_removal])\n",
    "        df_seq['angular_vel_x'] = angular_vel_seq_arr[:, 0]\n",
    "        df_seq['angular_vel_y'] = angular_vel_seq_arr[:, 1]\n",
    "        df_seq['angular_vel_z'] = angular_vel_seq_arr[:, 2]\n",
    "    else:\n",
    "        print(f\"Warning: Missing quaternion columns for angular velocity calculation in predict. Filling with 0.\")\n",
    "        df_seq['angular_vel_x'] = 0\n",
    "        df_seq['angular_vel_y'] = 0\n",
    "        df_seq['angular_vel_z'] = 0\n",
    "\n",
    "    # Calculate angular distance from quaternions in predict function\n",
    "    if all(col in df_seq.columns for col in rot_cols_for_gravity_removal):\n",
    "        angular_dist_seq_arr = calculate_angular_distance(df_seq[rot_cols_for_gravity_removal])\n",
    "        df_seq['angular_distance'] = angular_dist_seq_arr\n",
    "    else:\n",
    "        print(f\"Warning: Missing quaternion columns for angular distance calculation in predict. Filling with 0.\")\n",
    "        df_seq['angular_distance'] = 0\n",
    "\n",
    "\n",
    "    for i in range(1, 6): \n",
    "        pixel_cols_tof = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "        if not all(col in df_seq.columns for col in pixel_cols_tof):\n",
    "            print(f\"Warning: Missing some TOF pixel columns for tof_{i} in predict. Filling aggregates with 0.\")\n",
    "            df_seq[f'tof_{i}_mean'] = 0\n",
    "            df_seq[f'tof_{i}_std']  = 0\n",
    "            df_seq[f'tof_{i}_min']  = 0\n",
    "            df_seq[f'tof_{i}_max']  = 0\n",
    "            continue\n",
    "\n",
    "        tof_sensor_data = df_seq[pixel_cols_tof].replace(-1, np.nan)\n",
    "        df_seq[f'tof_{i}_mean'] = tof_sensor_data.mean(axis=1)\n",
    "        df_seq[f'tof_{i}_std']  = tof_sensor_data.std(axis=1)\n",
    "        df_seq[f'tof_{i}_min']  = tof_sensor_data.min(axis=1)\n",
    "        df_seq[f'tof_{i}_max']  = tof_sensor_data.max(axis=1)\n",
    "        \n",
    "    if 'tof_range_across_sensors' in final_feature_cols:\n",
    "        tof_mean_cols_for_contrast = [f'tof_{i}_mean' for i in range(1, 6) if f'tof_{i}_mean' in df_seq.columns]\n",
    "        thm_cols_for_contrast = [f'thm_{i}' for i in range(1, 6) if f'thm_{i}' in df_seq.columns]\n",
    "\n",
    "        if tof_mean_cols_for_contrast:\n",
    "            tof_values_for_contrast = df_seq[tof_mean_cols_for_contrast]\n",
    "            df_seq['tof_range_across_sensors'] = tof_values_for_contrast.max(axis=1) - tof_values_for_contrast.min(axis=1)\n",
    "            df_seq['tof_std_across_sensors'] = tof_values_for_contrast.std(axis=1)\n",
    "        else:\n",
    "            df_seq['tof_range_across_sensors'] = 0\n",
    "            df_seq['tof_std_across_sensors'] = 0\n",
    "\n",
    "        if thm_cols_for_contrast:\n",
    "            thm_values_for_contrast = df_seq[thm_cols_for_contrast]\n",
    "            df_seq['thm_range_across_sensors'] = thm_values_for_contrast.max(axis=1) - thm_values_for_contrast.min(axis=1)\n",
    "            df_seq['thm_std_across_sensors'] = thm_values_for_contrast.std(axis=1)\n",
    "        else:\n",
    "            df_seq['thm_range_across_sensors'] = 0\n",
    "            df_seq['thm_std_across_sensors'] = 0\n",
    "        \n",
    "    df_seq_final_features = pd.DataFrame(index=df_seq.index)\n",
    "    for col_name in final_feature_cols:\n",
    "        if col_name in df_seq.columns:\n",
    "            df_seq_final_features[col_name] = df_seq[col_name]\n",
    "        else:\n",
    "            print(f\"CRITICAL ERROR IN PREDICT: Feature '{col_name}' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\")\n",
    "            df_seq_final_features[col_name] = 0 \n",
    "            \n",
    "    mat_unscaled = df_seq_final_features.ffill().bfill().fillna(0).values.astype('float32')\n",
    "    \n",
    "    mat_scaled = scaler.transform(mat_unscaled)\n",
    "    \n",
    "    pad_input = pad_sequences([mat_scaled], maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n",
    "    \n",
    "    # ---------------------------------------------- #\n",
    "    # Blending Models\n",
    "    # ---------------------------------------------- #\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        idx = int(model.predict(pad_input, verbose=1).argmax(1)[0])\n",
    "        predictions.append(idx)\n",
    "    \n",
    "    idx = max(set(predictions), key=predictions.count)\n",
    "    return str(gesture_classes[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b9ae6d",
   "metadata": {
    "papermill": {
     "duration": 0.004841,
     "end_time": "2025-07-06T12:45:11.419513",
     "exception": false,
     "start_time": "2025-07-06T12:45:11.414672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 》》》**Submit Inference server**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59eab061",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-07-06T12:45:11.430418Z",
     "iopub.status.busy": "2025-07-06T12:45:11.429859Z",
     "iopub.status.idle": "2025-07-06T12:45:20.181554Z",
     "shell.execute_reply": "2025-07-06T12:45:20.180914Z"
    },
    "papermill": {
     "duration": 8.758373,
     "end_time": "2025-07-06T12:45:20.182769",
     "exception": false,
     "start_time": "2025-07-06T12:45:11.424396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectral Entropy for X: 3.7852467838952655\n",
      "Spectral Entropy for Y: 3.6761380892615727\n",
      "Spectral Entropy for Z: 3.7777093115696307\n",
      "CRITICAL ERROR IN PREDICT: Feature 'angular_vel_x' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'angular_vel_y' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'angular_vel_z' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'angular_distance' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_1_mean' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_1_std' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_1_min' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_1_max' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_2_mean' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_2_std' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_2_min' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_2_max' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_3_mean' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_3_std' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_3_min' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_3_max' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_4_mean' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_4_std' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_4_min' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_4_max' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_5_mean' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_5_std' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_5_min' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_5_max' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 12:45:12.571985: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "I0000 00:00:1751805913.741530      58 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 810ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 791ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 787ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 798ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 12:45:17.975230: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Spectral Entropy for X: 3.8599484093033185\n",
      "Spectral Entropy for Y: 3.6630476754975465\n",
      "Spectral Entropy for Z: 3.828456355532909\n",
      "CRITICAL ERROR IN PREDICT: Feature 'angular_vel_x' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'angular_vel_y' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'angular_vel_z' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'angular_distance' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_1_mean' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_1_std' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_1_min' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_1_max' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_2_mean' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_2_std' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_2_min' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_2_max' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_3_mean' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_3_std' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_3_min' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_3_max' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_4_mean' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_4_std' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_4_min' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_4_max' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_5_mean' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_5_std' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_5_min' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "CRITICAL ERROR IN PREDICT: Feature 'tof_5_max' expected by model (from final_feature_cols) was NOT generated in df_seq. Filling with 0. THIS IS LIKELY A BUG.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n"
     ]
    }
   ],
   "source": [
    "import kaggle_evaluation.cmi_inference_server\n",
    "inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        data_paths=(\n",
    "            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv',\n",
    "            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv',\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a3feaf",
   "metadata": {
    "papermill": {
     "duration": 0.006109,
     "end_time": "2025-07-06T12:45:20.195963",
     "exception": false,
     "start_time": "2025-07-06T12:45:20.189854",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**测试和评估**"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12518947,
     "sourceId": 102335,
     "sourceType": "competition"
    },
    {
     "datasetId": 7645099,
     "sourceId": 12139340,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7748073,
     "sourceId": 12293285,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 242954653,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 246893721,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 37.179953,
   "end_time": "2025-07-06T12:45:23.222477",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-06T12:44:46.042524",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
